{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPSMHP4ZH/zGqIyLGnlnFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/colab-notebooks/blob/main/Colab_Pyspark_H%26M_EDA_Recommendation_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTQe5CRCR7GG",
        "outputId": "eacfe5db-0966-4a73-a897-fc86a27b5ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Dataset URL: https://www.kaggle.com/datasets/odins0n/hm256x256\n",
            "License(s): other\n",
            "Downloading hm256x256.zip to /content\n",
            " 96% 2.05G/2.13G [00:12<00:02, 36.4MB/s]\n",
            "100% 2.13G/2.13G [00:12<00:00, 182MB/s] \n"
          ]
        }
      ],
      "source": [
        "#@title Install Kaggle modules and download the dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n",
        "!kaggle datasets download -d odins0n/hm256x256\n",
        "!unzip -q \"/content/hm256x256.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ql-SBs27TI3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7646018"
      },
      "source": [
        "âœ¨ **Getting Started** âœ¨\n",
        "\n",
        "Before we dive into building our recommendation system, we need to set up our environment. This involves installing the necessary libraries and downloading the dataset we'll be working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4669dfce",
        "outputId": "d4c98c3f-da98-4ccf-9774-3541c68a16e8"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8282150d"
      },
      "source": [
        "âš™ï¸ **Setting up Spark** âš™ï¸\n",
        "\n",
        "Before we can use PySpark, we need to start a Spark session. We'll also import some useful libraries that will help us work with data and build our recommendation model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
        "from pyspark.sql.functions import col,array_contains\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import udf,col,when\n",
        "from pyspark.sql.functions import to_timestamp,date_format\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "sc = SparkSession.builder.appName(\"Recommendations\").config(\"spark.sql.files.maxPartitionBytes\", 5000000).getOrCreate()\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "lUuDA2AYTg9X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d62a1c74"
      },
      "source": [
        "ðŸ’¾ **Loading the Transaction Data** ðŸ’¾\n",
        "\n",
        "We'll load the transaction data into a Spark DataFrame. This dataset contains all the purchase information, which is super important for our recommendation system."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transaction = spark.read.option(\"header\",True) \\\n",
        "              .csv(\"/content/transactions_train.csv\")\n",
        "transaction.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g00yZvVnTuIS",
        "outputId": "a26a0122-08b9-41f2-c212-4eb253dd22c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- t_dat: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- article_id: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- sales_channel_id: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d1ccef8"
      },
      "source": [
        "ðŸ“… **Checking the Date Range** ðŸ“…\n",
        "\n",
        "It's always good to know the time period our data covers. Let's find the earliest and latest transaction dates in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max\n",
        "from pyspark.sql.functions import unix_timestamp, lit\n",
        "min_date, max_date = transaction.select(min(\"t_dat\"), max(\"t_dat\")).first()\n",
        "min_date, max_date"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYUlKdr-T9To",
        "outputId": "1f97a9a4-46db-449f-e027-5582424b7060"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2018-09-20', '2020-09-22')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min as min_, max as max_\n",
        "\n",
        "# Cache the DataFrame if reused multiple times\n",
        "transaction.cache()\n",
        "\n",
        "# Aggregate min and max in one pass\n",
        "date_range = transaction.agg(\n",
        "    min_(\"t_dat\").alias(\"min_date\"),\n",
        "    max_(\"t_dat\").alias(\"max_date\")\n",
        ").collect()[0]\n",
        "\n",
        "min_date = date_range['min_date']\n",
        "max_date = date_range['max_date']\n",
        "print(min_date, max_date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vLkkVSgUi4K",
        "outputId": "87a0ccc6-38dd-4e9f-cc6d-882239bb35f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2018-09-20 2020-09-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ea85cb8"
      },
      "source": [
        "ðŸ§¹ **Preparing the Data** ðŸ§¹\n",
        "\n",
        "We'll clean and transform the transaction data to get it ready for our recommendation system. This includes filtering by date and counting how many times each customer bought a specific article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a527e48b"
      },
      "source": [
        "hm =  transaction.withColumn('t_dat', transaction['t_dat'].cast('string'))\n",
        "hm = hm.withColumn('date', from_unixtime(unix_timestamp('t_dat', 'yyyy-MM-dd')))\n",
        "hm = hm.withColumn('year', year(col('date')))\n",
        "hm = hm.withColumn('month', month(col('date')))\n",
        "hm = hm.withColumn('day', date_format(col('date'), \"d\"))\n",
        "\n",
        "hm = hm[hm['year'] == 2020]\n",
        "hm = hm[hm['month'] == 9]\n",
        "hm = hm[hm['day'] == 22]\n",
        "transaction.unpersist()\n",
        "\n",
        "# Prepare the dataset\n",
        "hm = hm.groupby('customer_id', 'article_id').count()\n",
        "hm.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}