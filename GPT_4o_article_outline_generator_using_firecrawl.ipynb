{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/colab-notebooks/blob/main/GPT_4o_article_outline_generator_using_firecrawl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab notebook is designed to help SEO professionals automate the process of generating article outlines optimized for specific keywords using GPT-4o and various Google APIs. By leveraging Google's NLP and Custom Search APIs, OpenAI's GPT-4o, and Weights & Biases, this notebook scrapes top-ranking web pages, extracts key information, and generates an optimized article outline based on key aspects of ranking sites.\n",
        "\n",
        "In this notebook, you will:\n",
        "\n",
        "* Input your target keywords and article type.\n",
        "* Use Google Custom Search API to find top-ranking pages for your keywords.\n",
        "* Scrape and analyze the content of these pages.\n",
        "* Extract key entities and questions using Google's NLP API.\n",
        "* Generate an optimized article outline using GPT-4o.\n",
        "\n",
        "You'll need ...\n",
        "\n",
        "**Accounts and API keys for:**\n",
        "\n",
        "* Custom Search and Cloud Natural Language APIs: A step-by-step guide can be found [here](https://wandb.ai/onlineinference/article_outlines_entities_questions/reports/Generating-content-outlines-with-prompt-engineering-entities-and-GPT-4o--Vmlldzo4Mjc1MDc1?utm_source=pubcon&utm_medium=colab&utm_campaign=daves_pubcon_demo#google-api-key). (very low cost)\n",
        "* OpenAI: [Sign up](https://platform.openai.com/docs/quickstart) and obtain your API key from the OpenAI dashboard. (very low cost with a free trial)\n",
        "* Firecrawl: [Sign up](https://www.firecrawl.dev/) and obtain your API key. (free option)\n",
        "* Weights & Biases: [Sign up](https://wandb.ai/site/?utm_source=pubcon&utm_medium=colab&utm_campaign=daves_pubcon_demo) and obtain your API key. (free option)"
      ],
      "metadata": {
        "id": "n9YgFw-ymPNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Define Your Target Keywords and Article Type\n",
        "\n",
        "In this step, you will define the primary and secondary keywords you are aiming to rank for, as well as the type of article you want to create."
      ],
      "metadata": {
        "id": "wbDDngkVnO68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWednVrtush1"
      },
      "outputs": [],
      "source": [
        "# Step 1: Define Your Target Keywords and Article Type\n",
        "\n",
        "# Define the primary term that you're trying to rank for.\n",
        "query = input(\"What do you want to rank for: \")\n",
        "\n",
        "# Define any secondary terms you're trying to rank for.\n",
        "query_secondary = input(\"Are there other terms you're trying to rank for (comma separated): \")\n",
        "\n",
        "# Define the type of article outline you want to create.\n",
        "article_type = input(\"What type of article is it (e.g., deep dive, quickstart, tutorial, etc.): \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Install and Import Necessary Libraries\n",
        "\n",
        "In this step, we will install and import all the necessary libraries required for the notebook."
      ],
      "metadata": {
        "id": "PoHZ5Mv6nYC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Install and Import Necessary Libraries\n",
        "\n",
        "# Install required packages\n",
        "!pip install --upgrade google-api-python-client google-cloud-language openai weave wandb firecrawl\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "from getpass import getpass\n",
        "from collections import defaultdict\n",
        "from google.cloud import language_v1\n",
        "from googleapiclient.discovery import build\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import time\n",
        "from firecrawl import FirecrawlApp\n",
        "\n",
        "# Import wandb and weave for logging and visualization\n",
        "import wandb\n",
        "import weave"
      ],
      "metadata": {
        "id": "jo3NRrOHne_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Set Up API Keys and Credentials\n",
        "\n",
        "In this step, you will input your API keys and set up credentials for the various services used in this notebook.\n",
        "\n",
        "Instructions on getting keys:\n",
        "\n",
        "**Google Cloud API Key and Credentials:**\n",
        "* Sign up for a Google Cloud account if you haven't already.\n",
        "* Enable the Google NLP API and the Custom Search API.\n",
        "* Generate an API key and download the JSON credentials file.\n",
        "\n",
        "**OpenAI API Key:**\n",
        "* Sign up for an OpenAI account and obtain your API key from the OpenAI dashboard.\n",
        "\n",
        "**Firecrawl API Key:**\n",
        "* Sign up for Firecrawl at Firecrawl's website and obtain your API key.\n",
        "\n",
        "**Weights & Biases:**\n",
        "* Sign up for a wandb account at wandb.ai and obtain your API key.\n",
        "* Once you've signed up you'll find it [here](https://wandb.ai/authorize).\n"
      ],
      "metadata": {
        "id": "Ii6SGuGonhaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Set Up API Keys and Credentials\n",
        "\n",
        "# Google API Key\n",
        "google_api = input(\"Enter your Google API Key: \")\n",
        "\n",
        "# Google Application Credentials (JSON file path)\n",
        "# In Colab, we need to upload the JSON file and set the environment variable\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your Google Application Credentials JSON file.\")\n",
        "uploaded = files.upload()\n",
        "credentials_filename = list(uploaded.keys())[0]\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_filename\n",
        "\n",
        "# Google Custom Search Engine ID\n",
        "google_search_id = input(\"Enter your Google Custom Search Engine ID (cx): \")\n",
        "\n",
        "\n",
        "# Firecrawl API Key\n",
        "firecrawl_api_key = getpass(\"Enter your Firecrawl API Key: \")\n",
        "app = FirecrawlApp(api_key=firecrawl_api_key)\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "wandb_api_key = getpass(\"Enter your wandb API Key: \")\n",
        "wandb.login(key=wandb_api_key)\n",
        "\n",
        "# OpenAI API Key\n",
        "openai_api_key = getpass(\"Enter your OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "id": "F-RyomeUnhug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Define the functions\n",
        "\n",
        "In this step, we define several functions that will be used throughout the notebook:\n",
        "\n",
        "* *google_search*: Performs a Google Custom Search.\n",
        "\n",
        "* *fetch_content_with_firecrawl*: Fetches content from a URL using Firecrawl.\n",
        "\n",
        "* *extract_headings_from_markdown*: Extracts headings from markdown text.\n",
        "\n",
        "* *generate_summary*: Generates a summary of the text using OpenAI's GPT-4o.\n",
        "\n",
        "* *extract_questions*: Extracts important questions from the text.\n",
        "\n",
        "* *top_questions*: Selects the top questions from a list.\n",
        "\n",
        "* *analyze_entities*: Analyzes entities in the text using Google Cloud NLP."
      ],
      "metadata": {
        "id": "-51u0bl2nh1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define Helper Functions\n",
        "\n",
        "# Setup Google Search API\n",
        "def google_search(search_term, api_key, cse_id, **kwargs):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
        "    return res['items']\n",
        "\n",
        "# Function to extract content from a webpage using Firecrawl\n",
        "def fetch_content_with_firecrawl(url):\n",
        "    try:\n",
        "        scrape_result = app.scrape_url(url, params={'formats': ['markdown']})\n",
        "        if '403 Forbidden' in scrape_result.get('status', ''):\n",
        "            print(f\"Access to {url} was denied with a 403 Forbidden error.\")\n",
        "            return None\n",
        "        page_text = scrape_result.get('markdown', '')\n",
        "        if not page_text:\n",
        "            print(f\"No content available for {url}\")\n",
        "            return None\n",
        "        return page_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching content from {url}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract headings from markdown text\n",
        "def extract_headings_from_markdown(markdown_text):\n",
        "    \"\"\"Extract headings from markdown text based on markdown syntax.\"\"\"\n",
        "    headings = []\n",
        "    for line in markdown_text.split('\\n'):\n",
        "        line = line.strip()\n",
        "        if line.startswith('#'):\n",
        "            # Remove leading '#' characters and any extra whitespace\n",
        "            heading = line.lstrip('#').strip()\n",
        "            if heading:\n",
        "                headings.append(heading)\n",
        "    return headings\n",
        "\n",
        "# Function to generate a summary of the text using OpenAI GPT-4o\n",
        "def generate_summary(text, headings):\n",
        "    \"\"\"Generate a GPT-4o summary of the text using the headings.\"\"\"\n",
        "    # Prepare the prompt\n",
        "    headings_text = '\\n'.join(f\"- {heading}\" for heading in headings)\n",
        "    prompt = (f\"Summarize the following article, focusing on these headings:\\n{headings_text}\\n\\n\"\n",
        "              f\"The summary should be concise (max 500 tokens) and capture the key points.\")\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert summarizer.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt + \"\\n\\n\" + text}\n",
        "            ],\n",
        "            model=\"gpt-4o\",\n",
        "            max_tokens=500,\n",
        "            temperature=0.5,\n",
        "            n=1\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating summary: {e}\")\n",
        "        return \"Summary not available.\"\n",
        "\n",
        "# Function to extract questions from the text using OpenAI GPT-4o\n",
        "def extract_questions(text):\n",
        "    \"\"\"Extract questions from the text using GPT-4o.\"\"\"\n",
        "    prompt = (f\"Extract important questions from the following text related to the query '{query}'. \"\n",
        "              f\"List them as bullet points.\\n\\n{text}\")\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant who extracts key questions from texts.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            model=\"gpt-4o\",\n",
        "            max_tokens=1000,\n",
        "            temperature=0.1,\n",
        "            n=1\n",
        "        )\n",
        "        questions_text = response.choices[0].message.content.strip()\n",
        "        # Split the response into individual questions based on bullet points\n",
        "        questions = re.findall(r\"-\\s*(.*)\", questions_text)\n",
        "        if not questions:\n",
        "            questions = [questions_text]\n",
        "        return questions\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting questions: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to select the top questions\n",
        "def top_questions(all_questions):\n",
        "    \"\"\"Generate the top questions from the list of all questions.\"\"\"\n",
        "    try:\n",
        "        questions_text = '\\n'.join(f\"- {question}\" for question in all_questions)\n",
        "        prompt = (f\"From the following list of questions extracted from top articles about '{query}', \"\n",
        "                  f\"select the 5 most important questions that would be most useful to the user. \"\n",
        "                  f\"List them as bullet points.\\n\\n{questions_text}\")\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an expert at identifying key questions on a topic.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            model=\"gpt-4o\",\n",
        "            max_tokens=500,\n",
        "            temperature=0.1,\n",
        "            n=1\n",
        "        )\n",
        "        top_questions_text = response.choices[0].message.content.strip()\n",
        "        # Split the response into individual questions based on bullet points\n",
        "        top_questions_list = re.findall(r\"-\\s*(.*)\", top_questions_text)\n",
        "        if not top_questions_list:\n",
        "            top_questions_list = [top_questions_text]\n",
        "        return top_questions_list\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating top questions: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to analyze entities using Google Cloud NLP\n",
        "def analyze_entities(text_content):\n",
        "    \"\"\"Analyze entities in the text using Google Cloud NLP.\"\"\"\n",
        "    try:\n",
        "        document = language_v1.Document(content=text_content, type_=language_v1.Document.Type.PLAIN_TEXT)\n",
        "        response = nlp_client.analyze_entities(document=document, encoding_type=language_v1.EncodingType.UTF8)\n",
        "        return response.entities\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing entities: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "ADx8rSU8nh_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Scrape and Analyze Top Ranking Pages\n",
        "\n",
        "In this step, we will:\n",
        "\n",
        "* Search for the top-ranking pages for your query using Google Custom Search API.\n",
        "* Scrape the content of these pages using Firecrawl.\n",
        "* Extract summaries, questions, and entities from the scraped content.\n",
        "* Log the data using Weights & Biases (wandb)."
      ],
      "metadata": {
        "id": "jzN3w0BJniHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Scrape and Analyze Top Ranking Pages\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Initialize Weights & Biases\n",
        "wandb.init(project=\"seo-content-strategy\")\n",
        "weave.init('seo-content-strategy')\n",
        "\n",
        "# Create W&B Tables to store scraped data\n",
        "firecrawl_table = wandb.Table(columns=[\n",
        "    \"url\",\n",
        "    \"markdown_summary\",\n",
        "    \"artifact_link\",\n",
        "    \"title\",\n",
        "    \"description\",\n",
        "    \"language\",\n",
        "    \"status_code\"\n",
        "])\n",
        "top_questions_table = wandb.Table(columns=[\n",
        "    \"question\"\n",
        "])\n",
        "\n",
        "entities_table = wandb.Table(columns=[\n",
        "    \"entity\",\n",
        "    \"aggregated_score\",\n",
        "    \"page_count\"\n",
        "])\n",
        "\n",
        "# Initialize a list to collect all questions\n",
        "all_questions = []\n",
        "entity_data = {}\n",
        "markdown_summaries = []\n",
        "\n",
        "# Initialize Google Cloud NLP client\n",
        "nlp_client = language_v1.LanguageServiceClient()\n",
        "\n",
        "# Search and scrape top 5 pages\n",
        "search_results = google_search(query, google_api, google_search_id, num=10)\n",
        "\n",
        "for result in search_results:\n",
        "    url = result['link']\n",
        "    print(f\"Processing URL: {url}\")\n",
        "    # Fetch content using Firecrawl\n",
        "    page_text = fetch_content_with_firecrawl(url)\n",
        "    if page_text is None:\n",
        "        print(f\"Failed to fetch content from {url}\")\n",
        "        continue  # Skip if no content\n",
        "\n",
        "    # Save the full content as a file\n",
        "    safe_title = ''.join(c if c.isalnum() else '_' for c in result.get('title', 'page_text'))\n",
        "    artifact_filename = f\"{safe_title}.txt\"\n",
        "    with open(artifact_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(page_text)\n",
        "\n",
        "    # Create and log the artifact\n",
        "    artifact = wandb.Artifact(name=f\"page_text_{safe_title}\", type='page_text')\n",
        "    artifact.add_file(artifact_filename)\n",
        "    artifact = wandb.run.log_artifact(artifact)  # Capture the logged artifact\n",
        "\n",
        "    # Wait for the artifact to be logged\n",
        "    artifact.wait()\n",
        "\n",
        "    # Get the artifact link\n",
        "    artifact_link = artifact.get_path(artifact_filename).ref_url\n",
        "\n",
        "    # Extract metadata\n",
        "    title = result.get('title', 'Unknown Title')\n",
        "    description = result.get('snippet', 'No description available')\n",
        "    language = 'en'  # Adjust as needed\n",
        "    status_code = 200  # Adjust as needed\n",
        "\n",
        "    # Extract headings from the markdown text\n",
        "    headings = extract_headings_from_markdown(page_text)\n",
        "\n",
        "    # Generate a summary using GPT-4\n",
        "    markdown_summary = generate_summary(page_text, headings)\n",
        "    if markdown_summary:\n",
        "        markdown_summaries.append(markdown_summary)\n",
        "    else:\n",
        "        print(f\"No summary generated for {url}\")\n",
        "\n",
        "    # Extract questions from the page and add them to the list\n",
        "    questions = extract_questions(page_text)\n",
        "    all_questions.extend(questions)\n",
        "\n",
        "    # Analyze entities in the page text\n",
        "    entities = analyze_entities(page_text)\n",
        "    page_entities = set()  # To track unique entities in this page\n",
        "\n",
        "    for entity in entities:\n",
        "        entity_name = entity.name\n",
        "        salience = entity.salience\n",
        "        # Update entity data\n",
        "        if entity_name in entity_data:\n",
        "            entity_info = entity_data[entity_name]\n",
        "            entity_info['total_salience'] += salience\n",
        "            if url not in entity_info['pages']:\n",
        "                entity_info['page_count'] += 1\n",
        "                entity_info['pages'].add(url)\n",
        "        else:\n",
        "            entity_data[entity_name] = {\n",
        "                'total_salience': salience,\n",
        "                'page_count': 1,\n",
        "                'pages': {url}\n",
        "            }\n",
        "\n",
        "    # Add data to the table, including the markdown summary and artifact link\n",
        "    firecrawl_table.add_data(\n",
        "        url,\n",
        "        markdown_summary,\n",
        "        artifact_link,\n",
        "        title,\n",
        "        description,\n",
        "        language,\n",
        "        status_code\n",
        "    )\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.remove(artifact_filename)\n",
        "\n",
        "# After processing all pages, generate the top questions\n",
        "top_questions_list = top_questions(all_questions)\n",
        "\n",
        "# Add the top questions to the table\n",
        "for question in top_questions_list:\n",
        "    top_questions_table.add_data(question)\n",
        "\n",
        "# Determine the top entities\n",
        "# Calculate a combined score: total_salience * page_count\n",
        "for entity_name, data in entity_data.items():\n",
        "    aggregated_score = data['total_salience'] * data['page_count']\n",
        "    data['aggregated_score'] = aggregated_score\n",
        "\n",
        "# Sort entities by the aggregated score\n",
        "top_entities = sorted(entity_data.items(), key=lambda item: item[1]['aggregated_score'], reverse=True)\n",
        "\n",
        "# Get the top N entities (e.g., top 10)\n",
        "top_n = 10\n",
        "top_entities = top_entities[:top_n]\n",
        "\n",
        "# Add top entities to the entities table\n",
        "for entity_name, data in top_entities:\n",
        "    entities_table.add_data(\n",
        "        entity_name,\n",
        "        data['aggregated_score'],\n",
        "        data['page_count']\n",
        "    )\n",
        "\n",
        "# Log the tables to W&B\n",
        "wandb.log({\n",
        "    \"scraped_data\": firecrawl_table,\n",
        "    \"top_questions_table\": top_questions_table,\n",
        "    \"entities_table\": entities_table,\n",
        "    \"markdown_summaries\": markdown_summaries,\n",
        "})\n",
        "\n",
        "print(\"Markdown Summaries:\", markdown_summaries)\n",
        "\n",
        "# Finish the W&B run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "Uwdhmx-WniOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Generate the Article Outline\n",
        " In this final step, we will generate an article outline using the collected data and OpenAI's GPT-4o."
      ],
      "metadata": {
        "id": "wdngibNzniXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the article outline using the collected data\n",
        "@weave.op()\n",
        "def generate_outline(top_entities, top_questions, query, query_secondary, article_type, markdown_summaries):\n",
        "    entities_str = ', '.join([entity_name for entity_name, _ in top_entities])\n",
        "    questions_str = ', '.join(top_questions)\n",
        "    summaries_str = '\\n\\n'.join(markdown_summaries)\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\",\n",
        "                     \"content\": \"You create succinct and clear article outlines. You can include your understanding \"\n",
        "                       \"of a topic to enhance an outline, but the focus should be on the inclusion of the entities, questions and top ranking content you are provided with.\"},\n",
        "                    {\"role\": \"assistant\", \"content\": \"You are a highly skilled writer, and you want to produce a \" + article_type +\n",
        "                       \" article outline that will appeal to users and rank well for queries given by the user. \"\n",
        "                       \"The outline will contain headings and sub-headings, with clear and concise descriptions of the content \"\n",
        "                       \"that is recommended for that section, and why. \\n When you are recommending to create an introductory paragraph to a section, to capture a featured snippet, \"\n",
        "                        \"note that it should be between 260 and 320 characters, and then provide a clearly noted example of what one might be. \"\n",
        "                         \"\\n After you have provided the outline, explain clearly how this article outline \"\n",
        "                       \"could be used to create an article that will rank well using best-practice SEO strategies as well as be helpful \"\n",
        "                       \"to users. You will be judged based on how well the article ranks, as well as how engaging the article is to readers, \"\n",
        "                       \"and provide the metrics you would suggest be used to judge whether you are successful. \\n An example of an article \"\n",
        "                       \"structure that works well is: \\n\\n\"\n",
        "                       \"Title: Top-Level Content (e.g., An Introduction to [Main Topic])\\n\\n\"\n",
        "                       \"The description: Visible on the page as well as used as the description to Google. Should be 130 <= character_count \"\n",
        "                       \"<= 160 and include the main keywords whenever possible.\\n\\n\"\n",
        "                       \"**Introduction**\\n\\n\"\n",
        "                       \"We don't need a heading tag here. Simply dive in with a brief description of what you'll be covering. One or two short \"\n",
        "                       \"paragraphs is great, but longer is fine.\\n\\n\"\n",
        "                       \"**H2 - Table Of Contents**\\n\"\n",
        "                       \"Ideally this section is done manually, but in a pinch, you can use the / Table Of Contents feature. You can add a bit of \"\n",
        "                       \"additional content below the table of contents if you like.\\n\\n\"\n",
        "                       \"**H2 - What Is [Main Topic]?**\\n\"\n",
        "                       \"Ideally here we have a section on the main topic. It will start with a paragraph that runs between 230 and 260 characters. \"\n",
        "                       \"This first paragraph should be the short answer to the question, and it will hopefully get picked up as a featured snippet \"\n",
        "                       \"and be used for voice search. It can and should be extended from there to provide additional information and context.\\n\\n\"\n",
        "                       \"**H2 - Optional - Subset Such As 'Types Of [Main Topic]'**\\n\"\n",
        "                       \"Brief description hereâ€”ideally 230 to 260 characters.\\n\\n\"\n",
        "                       \"**H3 - [Subset Type One] (e.g., First Subtype of [Main Topic])**\\n\"\n",
        "                       \"Description of [Subset Type One]. Ideally starting with 260 to 320 characters, but not mandatory, and expanding from there.\\n\\n\"\n",
        "                       \"**H3 - [Subset Type Two] (e.g., Second Subtype of [Main Topic])**\\n\"\n",
        "                       \"Description of [Subset Type Two]. Ideally starting with 260 to 320 characters, but not mandatory, and expanding from there.\\n\\n\"\n",
        "                        \"**H2 - A tutorial for [Main Topic]**\\n\"\n",
        "                       \"Generate a tutorial based on common macnmachine learning tasks, which are likely found in the summaries provided by the user. You should use W&B Weave whenever possible.\\n\\n\"\n",
        "                       \"**H2 - What Is [Main Topic] Used For?**\\n\"\n",
        "                       \"Again, ideally this starts with a 230 to 260 character short answer and is expanded upon.\\n\\n\"\n",
        "                       \"**H2 - Examples Of [Main Topic]** \\n\"\n",
        "                       \"Optionally, we can place a brief description of the types of examples. It should be done in H3 tags (assuming it's a simple one). \"\n",
        "                       \"A robust example requiring multiple stages (e.g., setup, running, visualizing) may require multiple H2 tags with H3s nested beneath.\\n\"\n",
        "                       \"**H2 - Recommended Reading On [Main Topic]** \\n\"\n",
        "                       \"Here we simply add a list with 2 or 4 articles you feel are related and would be of interest to the reader.\"},\n",
        "                    {\"role\": \"user\",\n",
        "            \"content\": \"Create an article outline that will rank well for \" + query + \" as the primary term, and \" + query_secondary +\n",
        "                       \" secondary keywords, which are less important but should still be considered. The following entities appear to be \"\n",
        "                       \"relevant to ranking in the top 10 and should be worked into the page:\\n\" + entities_str + \"\\n Try to ensure the outline \"\n",
        "                       \"will make it easy to work these into the article prominently and explain how this might be done in comments. Additionally, \"\n",
        "                       \"the following questions appear to be important to answer in the article:\\n\" + questions_str +\n",
        "                       \"\\n The following are summaries of the content and format that can be found on the top-ranking pages. This should heavily influence \"\n",
        "                       \"the outlines you produce, as this content ranks well: \\n\" + summaries_str + \"\\n\"\n",
        "                       \"Try to ensure that it will be easy to answer these questions in the article, and again, explain how you would recommend \"\n",
        "                       \"doing this in a way that will seem useful to the user. The article outline should begin by explaining \\n- all of the core \"\n",
        "                       \"concepts required to understand the topic\"\n",
        "        }],\n",
        "                max_tokens=2000,\n",
        "                temperature=0.2,\n",
        "                n=1\n",
        "            )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating outline: {e}\")\n",
        "        return \"Outline not available.\"\n",
        "\n",
        "# Generate the article outline\n",
        "article_outline = generate_outline(\n",
        "    top_entities,\n",
        "    top_questions_list,\n",
        "    query,\n",
        "    query_secondary,\n",
        "    article_type,\n",
        "    markdown_summaries\n",
        ")\n",
        "\n",
        "# Optionally, you can print or save the article outline\n",
        "print(\"Generated Article Outline:\")\n",
        "print(article_outline)\n"
      ],
      "metadata": {
        "id": "EIRW6wKjnigG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}