{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCSWlZ6vbKbMB11Pr4Gh5D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/colab-notebooks/blob/main/Colab_Pyspark_Churn_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bJpEvnAPnNQ",
        "outputId": "5e564a04-5757-426e-c1ea-7cdd1b401637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Dataset URL: https://www.kaggle.com/datasets/halimedogan/churn-dataset\n",
            "License(s): unknown\n",
            "Downloading churn-dataset.zip to /content\n",
            "  0% 0.00/262k [00:00<?, ?B/s]\n",
            "100% 262k/262k [00:00<00:00, 627MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Install Kaggle modules and download the dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n",
        "!kaggle datasets download -d halimedogan/churn-dataset\n",
        "!unzip -q \"/content/churn-dataset.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1388dce3"
      },
      "source": [
        "## ğŸ“Š Analyzing Customer Churn with PySpark\n",
        "\n",
        "Hey there! Let's dive into this customer churn dataset. You know the drill â€“ keeping existing customers happy is way more cost-effective than finding new ones. For us folks in the banking world, understanding *why* a client might pack their bags and leave is gold. If we can get ahead of churn, we can cook up some sweet loyalty programs and retention campaigns to keep our customers sticking around.\n",
        "\n",
        "Here's a quick rundown of what we're working with in this dataset, keeping an eye out for features that might be handy in our PySpark analysis:\n",
        "\n",
        "*   **Surname**: This is just a record ID, pretty much noise for our analysis. We won't be feeding this into our models.\n",
        "*   **CreditScore**: Looks like this one's just random. We can probably skip this when we're building our features for PySpark.\n",
        "*   **Geography**: This *could* be interesting. Location can definitely play a role in customer behavior. We'll want to consider this in our feature engineering â€“ maybe one-hot encoding for regions? ğŸŒ\n",
        "*   **Gender**: Let's see if gender has any sway. Another candidate for categorical feature handling in PySpark. ğŸš»\n",
        "*   **Age**: Definitely a key one! Older customers often seem more rooted. This is a strong candidate for our models. ğŸ‚\n",
        "*   **Tenure**: How long a customer's been with us. Makes sense that longer tenure might mean more loyalty. ğŸ•°ï¸\n",
        "*   **NumOfProducts**: How many banking products a customer uses. More products could imply more entanglement with the bank. ğŸ›’\n",
        "*   **HasCrCard**: Do they have a credit card with us? This might indicate a stronger tie to the bank. Another one to consider for feature vectors. ğŸ’³\n",
        "*   **IsActiveMember**: Are they actively using their account? Seems intuitive that active users are less likely to leave. âœ…\n",
        "*   **EstimatedSalary**: Like account balance, higher salaries might mean less likelihood to leave. We'll want to scale this one properly. ğŸ’°\n",
        "*   **Balance**: A really important feature! Customers with more cash in their accounts might be less likely to jump ship. This is probably a strong predictor. ğŸ’²\n",
        "*   **Exited** (Our Target Variable!): This is what we're trying to predict â€“ whether the customer churned or not. This will be our label column for training our PySpark machine learning models. ğŸ¯\n",
        "\n",
        "So, when we get into the PySpark code, we'll be focusing on transforming and vectorizing these features (Geography, Gender, Age, Tenure, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Balance) to build a model to predict `Exited`.\n",
        "\n",
        "Let's fire up the Spark context and get to it! ğŸ”¥\n",
        "\n",
        "<!-- Add an image here, for example: -->\n",
        "<!-- ![Churn Analysis](https://www.example.com/your-image.png) -->"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K-f_TfNUQyGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "600d4360"
      },
      "source": [
        "## ğŸ› ï¸ Setting up our PySpark Environment\n",
        "\n",
        "Before we can jump into analyzing the data and building our churn prediction model, we need to make sure we have all the necessary tools in place. This involves installing the `sparkmagic` and `pyspark` libraries, which are essential for running Spark code within our notebook. We'll also import a few other helpful libraries for data manipulation, visualization, and machine learning with PySpark.\n",
        "\n",
        "This setup ensures we have a smooth workflow as we move through the data preprocessing, feature engineering, and model training stages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6402384b",
        "outputId": "7935f6c2-c2a8-484b-eade-4c2af290a1c3"
      },
      "source": [
        "#@title Install and Import Libraries\n",
        "\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "\n",
        "# libraries\n",
        "import warnings\n",
        "# import findspark\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pyspark.ml.classification import GBTClassifier, LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder, StandardScaler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparkmagic\n",
            "  Downloading sparkmagic-0.22.0.tar.gz (45 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/45.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hdijupyterutils>=0.6 (from sparkmagic)\n",
            "  Downloading hdijupyterutils-0.22.0.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting autovizwidget>=0.6 (from sparkmagic)\n",
            "  Downloading autovizwidget-0.22.0.tar.gz (9.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (7.34.0)\n",
            "Requirement already satisfied: pandas<3.0.0 in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (2.32.3)\n",
            "Requirement already satisfied: ipykernel>=4.2.2 in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets>5.0.0 in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (7.7.1)\n",
            "Requirement already satisfied: notebook>=4.2 in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (6.5.7)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (6.4.2)\n",
            "Collecting requests_kerberos>=0.8.0 (from sparkmagic)\n",
            "  Downloading requests_kerberos-0.15.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nest_asyncio>1.5.5 in /usr/local/lib/python3.11/dist-packages (from sparkmagic) (1.6.0)\n",
            "Requirement already satisfied: plotly>=3 in /usr/local/lib/python3.11/dist-packages (from autovizwidget>=0.6->sparkmagic) (5.24.1)\n",
            "Collecting jupyter>=1 (from hdijupyterutils>=0.6->sparkmagic)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.2.2->sparkmagic) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.2.2->sparkmagic) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.2.2->sparkmagic) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.2.2->sparkmagic) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.2.2->sparkmagic) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.2.2->sparkmagic) (24.0.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.2.2->sparkmagic) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.2->sparkmagic) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.2->sparkmagic)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.2->sparkmagic) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.2->sparkmagic) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.2->sparkmagic) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.2->sparkmagic) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.2->sparkmagic) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.2->sparkmagic) (4.9.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>5.0.0->sparkmagic) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>5.0.0->sparkmagic) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>5.0.0->sparkmagic) (3.0.15)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (25.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (5.8.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.2->sparkmagic) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->sparkmagic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->sparkmagic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->sparkmagic) (2025.2)\n",
            "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.11/dist-packages (from requests_kerberos>=0.8.0->sparkmagic) (43.0.3)\n",
            "Collecting pyspnego[kerberos] (from requests_kerberos>=0.8.0->sparkmagic)\n",
            "  Downloading pyspnego-0.11.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->sparkmagic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->sparkmagic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->sparkmagic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->sparkmagic) (2025.6.15)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (1.17.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.2->sparkmagic) (0.8.4)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (6.1.0)\n",
            "Collecting jupyterlab (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic)\n",
            "  Downloading jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook>=4.2->sparkmagic) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.2->sparkmagic) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.2->sparkmagic) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.2->sparkmagic) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.2->sparkmagic) (4.24.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.2->sparkmagic) (0.7.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (9.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.2->sparkmagic) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0->sparkmagic) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.2->sparkmagic) (21.2.0)\n",
            "Collecting gssapi>=1.6.0 (from pyspnego[kerberos]->requests_kerberos>=0.8.0->sparkmagic)\n",
            "  Downloading gssapi-1.9.0.tar.gz (94 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.3/94.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting krb5>=0.3.0 (from pyspnego[kerberos]->requests_kerberos>=0.8.0->sparkmagic)\n",
            "  Downloading krb5-0.7.1.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.7/235.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.2->sparkmagic) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.2->sparkmagic) (1.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (2.22)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (0.25.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.2->sparkmagic) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.2->sparkmagic) (4.14.0)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic)\n",
            "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=1.8 (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic)\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (0.16.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=4.2.2->sparkmagic)\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (1.8.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.2.2->sparkmagic) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic)\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (1.3.1)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (6.0.2)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.2->sparkmagic)\n",
            "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading requests_kerberos-0.15.0-py2.py3-none-any.whl (12 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading jupyterlab-4.4.3-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspnego-0.11.2-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: sparkmagic, autovizwidget, hdijupyterutils, gssapi, krb5\n",
            "  Building wheel for sparkmagic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sparkmagic: filename=sparkmagic-0.22.0-py3-none-any.whl size=67563 sha256=fa94f58778ecac77e43fd3b856f52113039e2a2292b09fb51f5191bfd90ccc4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fa/31/53669d51c4ad68ca38903120b33cf1406be63ff13a452d532c\n",
            "  Building wheel for autovizwidget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autovizwidget: filename=autovizwidget-0.22.0-py3-none-any.whl size=14681 sha256=21b4e2c4afaa57f44fb57ecfc38b9c242d50e86b3e17f2844258b4d48232fed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/75/d3/1fea77d8da1d2b0f067e52dad4e34a825e389c3de0c8b0c75c\n",
            "  Building wheel for hdijupyterutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdijupyterutils: filename=hdijupyterutils-0.22.0-py3-none-any.whl size=7629 sha256=5ee6ab1176240f362ad0f8a31088a0266495d0531d304c394110bc26be0303cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/99/34/62aadc3f4cf30eb5e1d12222a503ab562acae7f12cc54a03d5\n",
            "  Building wheel for gssapi (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gssapi: filename=gssapi-1.9.0-cp311-cp311-linux_x86_64.whl size=4105311 sha256=eaeca2bad7ca3212138ea1b832fa0296bdccd340e1f1775b3575044229bd0db5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/79/8b/6300aa69684280a0de217fd573097478c724cb0ce75e9fab73\n",
            "  Building wheel for krb5 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for krb5: filename=krb5-0.7.1-cp311-cp311-linux_x86_64.whl size=6448498 sha256=619869d5f3c90807aee7772b43de763925983c7a44eae478bc5289e5fafbb02f\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/4c/fe/e39ec81c33db263e0e0d963af4c93b564f5deeedef93e4bbdb\n",
            "Successfully built sparkmagic autovizwidget hdijupyterutils gssapi krb5\n",
            "Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, krb5, json5, jedi, gssapi, fqdn, async-lru, jupyter-server-terminals, jupyter-client, arrow, pyspnego, isoduration, requests_kerberos, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter, hdijupyterutils, autovizwidget, sparkmagic\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "Successfully installed arrow-1.3.0 async-lru-2.0.5 autovizwidget-0.22.0 fqdn-1.5.1 gssapi-1.9.0 hdijupyterutils-0.22.0 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.3 jupyterlab-server-2.27.3 krb5-0.7.1 overrides-7.7.0 pyspnego-0.11.2 python-json-logger-3.3.0 requests_kerberos-0.15.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 sparkmagic-0.22.0 types-python-dateutil-2.9.0.20250516 uri-template-1.3.0\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d7a0063"
      },
      "source": [
        "## ğŸ”¥ Firing up Spark and Loading the Data\n",
        "\n",
        "Now that our libraries are in place, it's time to get Spark going! We'll create a Spark session, which is the entry point for any Spark functionality. Then, we'll load our churn dataset (`churn2.csv`) into a Spark DataFrame. This will allow us to leverage Spark's distributed processing capabilities for our analysis. Finally, we'll take a quick peek at the first few rows to make sure everything loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "886a5732",
        "outputId": "10f99260-f721-41b4-d1ee-fb20e595834e"
      },
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark_df = spark.read.csv('/content/churn2.csv', inferSchema=True, header=True)\n",
        "spark_df.show(10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "|RowNumber|CustomerId| Surname|CreditScore|Geography|Gender|Age|Tenure|  Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "|        1|  15634602|Hargrave|        619|   France|Female| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|\n",
            "|        2|  15647311|    Hill|        608|    Spain|Female| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|\n",
            "|        3|  15619304|    Onio|        502|   France|Female| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|\n",
            "|        4|  15701354|    Boni|        699|   France|Female| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|\n",
            "|        5|  15737888|Mitchell|        850|    Spain|Female| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|\n",
            "|        6|  15574012|     Chu|        645|    Spain|  Male| 44|     8|113755.78|            2|        1|             0|      149756.71|     1|\n",
            "|        7|  15592531|Bartlett|        822|   France|  Male| 50|     7|      0.0|            2|        1|             1|        10062.8|     0|\n",
            "|        8|  15656148|  Obinna|        376|  Germany|Female| 29|     4|115046.74|            4|        1|             0|      119346.88|     1|\n",
            "|        9|  15792365|      He|        501|   France|  Male| 44|     4|142051.07|            2|        0|             1|        74940.5|     0|\n",
            "|       10|  15592389|      H?|        684|   France|  Male| 27|     2|134603.88|            1|        1|             1|       71725.73|     0|\n",
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed5a39cc"
      },
      "source": [
        "## ğŸ‘€ Getting to Know Our Data: Initial Exploration\n",
        "\n",
        "With our Spark DataFrame ready, it's time for some initial exploration. We'll start by checking the dimensions of the dataset (number of rows and columns), examining the schema to understand the data types of each column, and generating some descriptive statistics. This gives us a foundational understanding of our data before we dive deeper into feature engineering and modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74c8364f",
        "outputId": "1ef72b0c-81ae-4831-8dcc-02d5a1dfed4b"
      },
      "source": [
        "print(\"Shape: \", (spark_df.count(), len(spark_df.columns)))\n",
        "spark_df.printSchema()\n",
        "spark_df.describe().show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape:  (10000, 14)\n",
            "root\n",
            " |-- RowNumber: integer (nullable = true)\n",
            " |-- CustomerId: integer (nullable = true)\n",
            " |-- Surname: string (nullable = true)\n",
            " |-- CreditScore: integer (nullable = true)\n",
            " |-- Geography: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Tenure: integer (nullable = true)\n",
            " |-- Balance: double (nullable = true)\n",
            " |-- NumOfProducts: integer (nullable = true)\n",
            " |-- HasCrCard: integer (nullable = true)\n",
            " |-- IsActiveMember: integer (nullable = true)\n",
            " |-- EstimatedSalary: double (nullable = true)\n",
            " |-- Exited: integer (nullable = true)\n",
            "\n",
            "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
            "|summary|         RowNumber|       CustomerId|Surname|      CreditScore|Geography|Gender|               Age|            Tenure|          Balance|     NumOfProducts|          HasCrCard|     IsActiveMember|  EstimatedSalary|             Exited|\n",
            "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
            "|  count|             10000|            10000|  10000|            10000|    10000| 10000|             10000|             10000|            10000|             10000|              10000|              10000|            10000|              10000|\n",
            "|   mean|            5000.5|  1.56909405694E7|   NULL|         650.5288|     NULL|  NULL|           38.9218|            5.0128|76485.88928799961|            1.5302|             0.7055|             0.5151|100090.2398809998|             0.2037|\n",
            "| stddev|2886.8956799071675|71936.18612274907|   NULL|96.65329873613035|     NULL|  NULL|10.487806451704587|2.8921743770496837|62397.40520238599|0.5816543579989917|0.45584046447513327|0.49979692845891815|57510.49281769821|0.40276858399486065|\n",
            "|    min|                 1|         15565701|  Abazu|              350|   France|Female|                18|                 0|              0.0|                 1|                  0|                  0|            11.58|                  0|\n",
            "|    max|             10000|         15815690| Zuyeva|              850|    Spain|  Male|                92|                10|        250898.09|                 4|                  1|                  1|        199992.48|                  1|\n",
            "+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qd6o-5RgTHIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f0893ee"
      },
      "source": [
        "## ğŸ“Š Checking the Target Variable Distribution\n",
        "\n",
        "Before we build any models, it's always a good idea to understand the distribution of our target variable. In this case, we want to see the split between customers who `Exited` (churned) and those who didn't. This helps us understand if we have a balanced dataset or if we need to consider techniques for handling imbalanced data later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7a6f715",
        "outputId": "367d6abf-b284-47f8-e166-91c958072a22"
      },
      "source": [
        "spark_df.groupby(\"exited\").count().show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|exited|count|\n",
            "+------+-----+\n",
            "|     1| 2037|\n",
            "|     0| 7963|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fed078de"
      },
      "source": [
        "## Exploring Tenure and Churn\n",
        "\n",
        "Let's see if the length of time a customer has been with the bank (their `Tenure`) has any influence on whether they churn (`Exited`). We'll calculate the average tenure for customers who stayed versus those who left to see if there's a noticeable difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b492c98a",
        "outputId": "a8872d91-8d11-4284-a0be-69c2ce812f72"
      },
      "source": [
        "spark_df.groupby(\"exited\").agg({\"tenure\": \"mean\"}).show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+\n",
            "|exited|      avg(tenure)|\n",
            "+------+-----------------+\n",
            "|     1|4.932744231713304|\n",
            "|     0|5.033278914981791|\n",
            "+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc2c635"
      },
      "source": [
        "## Delving into Numerical Features\n",
        "\n",
        "Now, let's focus on our numerical features. We'll select these columns and get a quick snapshot of their descriptive statistics. This helps us understand the spread and central tendency of these important variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83886a55",
        "outputId": "01885c08-ae06-4d58-a04b-245d1b88f749"
      },
      "source": [
        "#Selection and summary statistics of all numeric variables\n",
        "num_cols = [col[0] for col in spark_df.dtypes if col[1] != 'string']\n",
        "spark_df.select(num_cols).describe().show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
            "|summary|         RowNumber|       CustomerId|      CreditScore|               Age|            Tenure|          Balance|     NumOfProducts|          HasCrCard|     IsActiveMember|  EstimatedSalary|             Exited|\n",
            "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
            "|  count|             10000|            10000|            10000|             10000|             10000|            10000|             10000|              10000|              10000|            10000|              10000|\n",
            "|   mean|            5000.5|  1.56909405694E7|         650.5288|           38.9218|            5.0128|76485.88928799961|            1.5302|             0.7055|             0.5151|100090.2398809998|             0.2037|\n",
            "| stddev|2886.8956799071675|71936.18612274907|96.65329873613035|10.487806451704587|2.8921743770496837|62397.40520238599|0.5816543579989917|0.45584046447513327|0.49979692845891815|57510.49281769821|0.40276858399486065|\n",
            "|    min|                 1|         15565701|              350|                18|                 0|              0.0|                 1|                  0|                  0|            11.58|                  0|\n",
            "|    max|             10000|         15815690|              850|                92|                10|        250898.09|                 4|                  1|                  1|        199992.48|                  1|\n",
            "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "097040d5"
      },
      "source": [
        "## Exploring Categorical Features\n",
        "\n",
        "Let's now turn our attention to the categorical variables in our dataset. We'll select these columns and examine their summary statistics to understand the different categories present and their frequencies. This is a crucial step before we encode these features for our machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "089e6c5e",
        "outputId": "28af074d-da98-4d2f-b5a0-91dd43762736"
      },
      "source": [
        "#Selection and summary of all categorical variables\n",
        "cat_cols = [col[0] for col in spark_df.dtypes if col[1] == 'string']\n",
        "spark_df.select(cat_cols).describe().show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+---------+------+\n",
            "|summary|Surname|Geography|Gender|\n",
            "+-------+-------+---------+------+\n",
            "|  count|  10000|    10000| 10000|\n",
            "|   mean|   NULL|     NULL|  NULL|\n",
            "| stddev|   NULL|     NULL|  NULL|\n",
            "|    min|  Abazu|   France|Female|\n",
            "|    max| Zuyeva|    Spain|  Male|\n",
            "+-------+-------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3135116"
      },
      "source": [
        "## Analyzing Numerical Features by Churn Status\n",
        "\n",
        "Let's investigate how the average values of our numerical features differ between customers who churned (`Exited`=1) and those who didn't (`Exited`=0). This can highlight which numerical variables show a significant difference between the two groups and might be strong predictors of churn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a597166f",
        "outputId": "b86f5555-9fcb-4cd9-9466-42a1687fe956"
      },
      "source": [
        "# mean of numerical variables relative to the target variable\n",
        "for col in [col.lower() for col in num_cols]:\n",
        "    spark_df.groupby(\"exited\").agg({col: \"mean\"}).show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+\n",
            "|exited|   avg(rownumber)|\n",
            "+------+-----------------+\n",
            "|     1|4905.917525773196|\n",
            "|     0|5024.694964209469|\n",
            "+------+-----------------+\n",
            "\n",
            "+------+--------------------+\n",
            "|exited|     avg(customerid)|\n",
            "+------+--------------------+\n",
            "|     1|1.5690051964653904E7|\n",
            "|     0|1.5691167881702876E7|\n",
            "+------+--------------------+\n",
            "\n",
            "+------+-----------------+\n",
            "|exited| avg(creditscore)|\n",
            "+------+-----------------+\n",
            "|     1|645.3514972999509|\n",
            "|     0|651.8531960316463|\n",
            "+------+-----------------+\n",
            "\n",
            "+------+-----------------+\n",
            "|exited|         avg(age)|\n",
            "+------+-----------------+\n",
            "|     1| 44.8379970544919|\n",
            "|     0|37.40838879819164|\n",
            "+------+-----------------+\n",
            "\n",
            "+------+-----------------+\n",
            "|exited|      avg(tenure)|\n",
            "+------+-----------------+\n",
            "|     1|4.932744231713304|\n",
            "|     0|5.033278914981791|\n",
            "+------+-----------------+\n",
            "\n",
            "+------+-----------------+\n",
            "|exited|     avg(balance)|\n",
            "+------+-----------------+\n",
            "|     1|91108.53933726063|\n",
            "|     0|72745.29677885193|\n",
            "+------+-----------------+\n",
            "\n",
            "+------+------------------+\n",
            "|exited|avg(numofproducts)|\n",
            "+------+------------------+\n",
            "|     1|1.4752086401570939|\n",
            "|     0|1.5442672359663443|\n",
            "+------+------------------+\n",
            "\n",
            "+------+------------------+\n",
            "|exited|    avg(hascrcard)|\n",
            "+------+------------------+\n",
            "|     1|0.6990672557682867|\n",
            "|     0|0.7071455481602411|\n",
            "+------+------------------+\n",
            "\n",
            "+------+-------------------+\n",
            "|exited|avg(isactivemember)|\n",
            "+------+-------------------+\n",
            "|     1|0.36082474226804123|\n",
            "|     0| 0.5545648624890117|\n",
            "+------+-------------------+\n",
            "\n",
            "+------+--------------------+\n",
            "|exited|avg(estimatedsalary)|\n",
            "+------+--------------------+\n",
            "|     1|   101465.6775306824|\n",
            "|     0|   99738.39177194514|\n",
            "+------+--------------------+\n",
            "\n",
            "+------+-----------+\n",
            "|exited|avg(exited)|\n",
            "+------+-----------+\n",
            "|     1|        1.0|\n",
            "|     0|        0.0|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3cefc47"
      },
      "source": [
        "## Checking for Missing Values\n",
        "\n",
        "Data cleaning is a vital step before we can train any machine learning model. Let's check if there are any missing values in our dataset. Identifying and handling these nulls is important for ensuring the quality of our data and the performance of our model. We'll count the number of missing values in each column and display the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ae977f42",
        "outputId": "e793ea69-610b-42b6-f85a-db750e8fe4e5"
      },
      "source": [
        "#Missing Values\n",
        "from pyspark.sql.functions import when, count, col\n",
        "spark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns]).toPandas().T"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0\n",
              "RowNumber        0\n",
              "CustomerId       0\n",
              "Surname          0\n",
              "CreditScore      0\n",
              "Geography        0\n",
              "Gender           0\n",
              "Age              0\n",
              "Tenure           0\n",
              "Balance          0\n",
              "NumOfProducts    0\n",
              "HasCrCard        0\n",
              "IsActiveMember   0\n",
              "EstimatedSalary  0\n",
              "Exited           0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69848e0d-d0d7-4177-897b-75e24ea1aa8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>RowNumber</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CustomerId</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Surname</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CreditScore</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Geography</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Gender</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tenure</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Balance</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NumOfProducts</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HasCrCard</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>IsActiveMember</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Exited</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69848e0d-d0d7-4177-897b-75e24ea1aa8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69848e0d-d0d7-4177-897b-75e24ea1aa8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69848e0d-d0d7-4177-897b-75e24ea1aa8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ea45cd66-9f5f-433e-bb92-1ce0b5d71bc8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ea45cd66-9f5f-433e-bb92-1ce0b5d71bc8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ea45cd66-9f5f-433e-bb92-1ce0b5d71bc8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"spark_df\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0228974"
      },
      "source": [
        "## Standardizing Column Names\n",
        "\n",
        "To ensure consistency and make it easier to work with our DataFrame, let's convert all column names to lowercase. This is a common practice in data processing pipelines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101ef299",
        "outputId": "779bcecb-8db6-4780-fe59-6ccf4308a178"
      },
      "source": [
        "spark_df = spark_df.toDF(*[c.lower() for c in spark_df.columns])\n",
        "spark_df.show(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "|rownumber|customerid| surname|creditscore|geography|gender|age|tenure|  balance|numofproducts|hascrcard|isactivemember|estimatedsalary|exited|\n",
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "|        1|  15634602|Hargrave|        619|   France|Female| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|\n",
            "|        2|  15647311|    Hill|        608|    Spain|Female| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|\n",
            "|        3|  15619304|    Onio|        502|   France|Female| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|\n",
            "|        4|  15701354|    Boni|        699|   France|Female| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|\n",
            "|        5|  15737888|Mitchell|        850|    Spain|Female| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|\n",
            "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bf5629"
      },
      "source": [
        "## Feature Interaction\n",
        "\n",
        "Let's move on to creating some new features! We'll start by dropping columns that aren't relevant for our model. Then, we'll engineer some interaction terms by combining existing features. These new features might help our model pick up on more nuanced patterns in the data that could be related to churn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8971ef96",
        "outputId": "a724d839-2fc1-4d43-d655-18dd1f41bfa4"
      },
      "source": [
        "# Feature Interaction\n",
        "spark_df = spark_df.drop('rownumber', \"customerid\", \"surname\")\n",
        "spark_df = spark_df.withColumn('creditscore_salary', spark_df.creditscore / spark_df.estimatedsalary)\n",
        "spark_df = spark_df.withColumn('creditscore_tenure', spark_df.creditscore * spark_df.tenure)\n",
        "spark_df = spark_df.withColumn('balance_salary', spark_df.balance / spark_df.estimatedsalary)\n",
        "spark_df.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+\n",
            "|creditscore|geography|gender|age|tenure|  balance|numofproducts|hascrcard|isactivemember|estimatedsalary|exited|  creditscore_salary|creditscore_tenure|    balance_salary|\n",
            "+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+\n",
            "|        619|   France|Female| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|0.006107615594765329|              1238|               0.0|\n",
            "|        608|    Spain|Female| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|0.005402399696186101|               608|0.7446769036217226|\n",
            "|        502|   France|Female| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|0.004406153623618106|              4016|1.4013745268322026|\n",
            "|        699|   France|Female| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|0.007449910542454738|               699|               0.0|\n",
            "|        850|    Spain|Female| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|0.010748051757559357|              1700|1.5870550464631954|\n",
            "+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2db46156"
      },
      "source": [
        "## Bucketization: Transforming Age into Categories\n",
        "\n",
        "To potentially capture different patterns across age groups, let's transform the numerical `age` feature into categorical bins. We'll define specific age ranges and assign each customer to an `age_cat` based on their age. This can sometimes help the model by providing a different perspective on this feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "423160cf",
        "outputId": "5ec0f95e-043d-4195-c56d-70a6b90c4cc8"
      },
      "source": [
        "#Bucketization / Bining / Num to Cat\n",
        "spark_df.select('age').describe().toPandas().transpose()\n",
        "spark_df.select(\"age\").summary(\"count\", \"min\", \"25%\", \"50%\",\"75%\", \"max\").show()\n",
        "bucketizer = Bucketizer(splits=[0, 35, 55, 75, 95], inputCol=\"age\", outputCol=\"age_cat\")\n",
        "spark_df = bucketizer.setHandleInvalid(\"keep\").transform(spark_df)\n",
        "spark_df = spark_df.withColumn('age_cat', spark_df.age_cat + 1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|summary|  age|\n",
            "+-------+-----+\n",
            "|  count|10000|\n",
            "|    min|   18|\n",
            "|    25%|   32|\n",
            "|    50%|   37|\n",
            "|    75%|   44|\n",
            "|    max|   92|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba7fe591"
      },
      "source": [
        "## Converting Age Categories to Integer Type\n",
        "\n",
        "We've successfully bucketized the `age` column into `age_cat`. Since these categories are discrete, let's convert the data type of the `age_cat` column from float to integer for better data representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79c91e32"
      },
      "source": [
        "#converting float values to integer\n",
        "spark_df = spark_df.withColumn(\"age_cat\", spark_df[\"age_cat\"].cast(\"integer\"))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4edfa6ff"
      },
      "source": [
        "## Encoding Categorical Features: Gender\n",
        "\n",
        "Now it's time to handle our categorical features. We'll start by encoding the `gender` column. Using PySpark's `StringIndexer`, we'll convert the 'Male' and 'Female' categories into numerical labels. This is a necessary step before we can use this feature in our machine learning model. We'll also convert the resulting column to an integer type and drop the original string column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9016b82a",
        "outputId": "ac748888-18be-4a40-cb67-0168ee5c6fe8"
      },
      "source": [
        "indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_label\")\n",
        "indexer.fit(spark_df).transform(spark_df).show(5)\n",
        "temp_sdf = indexer.fit(spark_df).transform(spark_df)\n",
        "spark_df = temp_sdf.withColumn(\"gender_label\", temp_sdf[\"gender_label\"].cast(\"integer\"))\n",
        "spark_df = spark_df.drop('gender')\n",
        "spark_df.show(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+\n",
            "|creditscore|geography|gender|age|tenure|  balance|numofproducts|hascrcard|isactivemember|estimatedsalary|exited|  creditscore_salary|creditscore_tenure|    balance_salary|age_cat|gender_label|\n",
            "+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+\n",
            "|        619|   France|Female| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|0.006107615594765329|              1238|               0.0|      2|         1.0|\n",
            "|        608|    Spain|Female| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|0.005402399696186101|               608|0.7446769036217226|      2|         1.0|\n",
            "|        502|   France|Female| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|0.004406153623618106|              4016|1.4013745268322026|      2|         1.0|\n",
            "|        699|   France|Female| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|0.007449910542454738|               699|               0.0|      2|         1.0|\n",
            "|        850|    Spain|Female| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|0.010748051757559357|              1700|1.5870550464631954|      2|         1.0|\n",
            "+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------+---------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+\n",
            "|creditscore|geography|age|tenure|  balance|numofproducts|hascrcard|isactivemember|estimatedsalary|exited|  creditscore_salary|creditscore_tenure|    balance_salary|age_cat|gender_label|\n",
            "+-----------+---------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+\n",
            "|        619|   France| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|0.006107615594765329|              1238|               0.0|      2|           1|\n",
            "|        608|    Spain| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|0.005402399696186101|               608|0.7446769036217226|      2|           1|\n",
            "|        502|   France| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|0.004406153623618106|              4016|1.4013745268322026|      2|           1|\n",
            "|        699|   France| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|0.007449910542454738|               699|               0.0|      2|           1|\n",
            "|        850|    Spain| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|0.010748051757559357|              1700|1.5870550464631954|      2|           1|\n",
            "+-----------+---------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35502c37"
      },
      "source": [
        "## Encoding Categorical Features: Geography\n",
        "\n",
        "Following the same process as with 'gender', we'll now encode the `geography` column. We'll use `StringIndexer` to map the different countries to numerical indices. This is another crucial step to prepare this categorical feature for our machine learning model. We'll then convert the resulting column to an integer type and remove the original string column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "109feb62",
        "outputId": "ca596736-8cdb-4e3d-adf0-57deda069100"
      },
      "source": [
        "indexer = StringIndexer(inputCol=\"geography\", outputCol=\"geography_label\")\n",
        "indexer.fit(spark_df).transform(spark_df).show(5)\n",
        "temp_sdf = indexer.fit(spark_df).transform(spark_df)\n",
        "spark_df = temp_sdf.withColumn(\"geography_label\", temp_sdf[\"geography_label\"].cast(\"integer\"))\n",
        "spark_df = spark_df.drop('geography')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+---------------+\n",
            "|creditscore|geography|age|tenure|  balance|numofproducts|hascrcard|isactivemember|estimatedsalary|exited|  creditscore_salary|creditscore_tenure|    balance_salary|age_cat|gender_label|geography_label|\n",
            "+-----------+---------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+---------------+\n",
            "|        619|   France| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|0.006107615594765329|              1238|               0.0|      2|           1|            0.0|\n",
            "|        608|    Spain| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|0.005402399696186101|               608|0.7446769036217226|      2|           1|            2.0|\n",
            "|        502|   France| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|0.004406153623618106|              4016|1.4013745268322026|      2|           1|            0.0|\n",
            "|        699|   France| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|0.007449910542454738|               699|               0.0|      2|           1|            0.0|\n",
            "|        850|    Spain| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|0.010748051757559357|              1700|1.5870550464631954|      2|           1|            2.0|\n",
            "+-----------+---------+---+------+---------+-------------+---------+--------------+---------------+------+--------------------+------------------+------------------+-------+------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}