{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/colab-notebooks/blob/main/Colab_Pyspark_OneVsRest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zajNF6uHZ9jo",
        "outputId": "9fb7de3d-8b9c-41f0-86aa-812fa1343459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Downloading facebook-recruiting-iii-keyword-extraction.zip to /content\n",
            "100% 2.90G/2.90G [00:40<00:00, 59.0MB/s]\n",
            "100% 2.90G/2.90G [00:40<00:00, 76.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Install Kaggle modules and download the dataset\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n",
        "!kaggle competitions download -c facebook-recruiting-iii-keyword-extraction\n",
        "!unzip -q \"/content/facebook-recruiting-iii-keyword-extraction.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi1QaB6vboxZ"
      },
      "outputs": [],
      "source": [
        "!unzip -q \"/content/Test.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg8AU6BPbsYt"
      },
      "outputs": [],
      "source": [
        "!unzip -q \"/content/Train.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7XSjDK8ybvlu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat_ws, col, split\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression, OneVsRest, OneVsRestModel\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# 1. Start Spark session\n",
        "spark = SparkSession.builder.appName(\"KeywordExtraction\").getOrCreate()\n",
        "\n",
        "# 2. Load train dataset\n",
        "train_df = spark.read.csv(\"Train.csv\", header=True, inferSchema=True, multiLine=True, escape='\"')\n",
        "train_df = train_df.na.drop(subset=[\"Title\", \"Body\", \"Tags\"])\n",
        "\n",
        "# 3. Combine Title and Body into one text column\n",
        "train_df = train_df.withColumn(\"text\", concat_ws(\" \", \"Title\", \"Body\"))\n",
        "\n",
        "# 4. Simplify: Use first tag only (for single-label classification)\n",
        "train_df = train_df.withColumn(\"tag\", split(col(\"Tags\"), \" \").getItem(0))\n",
        "\n",
        "# 5. StringIndexer for tags\n",
        "label_indexer = StringIndexer(inputCol=\"tag\", outputCol=\"label\").fit(train_df)\n",
        "\n",
        "# 6. NLP Pipeline: Tokenizer â†’ StopWordsRemover â†’ TF-IDF\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=10000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "# 7. Base classifier and OneVsRest wrapper\n",
        "lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.1)\n",
        "ovr = OneVsRest(classifier=lr, labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "# 8. Pipeline: from raw text to model\n",
        "pipeline = Pipeline(stages=[\n",
        "    tokenizer,\n",
        "    stopwords_remover,\n",
        "    hashing_tf,\n",
        "    idf,\n",
        "    label_indexer,\n",
        "    ovr\n",
        "])\n",
        "\n",
        "# 9. Train-test split\n",
        "train_data, test_data = train_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# 10. Fit model\n",
        "model = pipeline.fit(train_data)\n",
        "\n",
        "# 11. Predict and evaluate on test\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
        "f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
        "precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
        "recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
        "\n",
        "# Print all metrics\n",
        "print(\"âœ… Evaluation Metrics (OneVsRest Classifier):\")\n",
        "print(f\"â€¢ Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"â€¢ F1-score:  {f1:.4f}\")\n",
        "print(f\"â€¢ Precision: {precision:.4f}\")\n",
        "print(f\"â€¢ Recall:    {recall:.4f}\")\n",
        "\n",
        "# 12. Save the model\n",
        "model.write().overwrite().save(\"keyword_ovr_model\")\n",
        "print(\"ðŸ’¾ Model saved to 'keyword_ovr_model'\")\n",
        "\n",
        "# 13. Load test.csv (without Tags)\n",
        "test_df = spark.read.csv(\"Test.csv\", header=True, inferSchema=True, multiLine=True, escape='\"')\n",
        "test_df = test_df.na.drop(subset=[\"Title\", \"Body\"])\n",
        "test_df = test_df.withColumn(\"text\", concat_ws(\" \", \"Title\", \"Body\"))\n",
        "\n",
        "# 14. Load trained model and predict\n",
        "loaded_model = Pipeline.load(\"keyword_ovr_model\")\n",
        "test_predictions = loaded_model.transform(test_df)\n",
        "\n",
        "# 15. Output predictions with IDs\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "predicted_labels = test_predictions.select(\"Id\", \"prediction\")\n",
        "\n",
        "# Optional: Map predictions back to original tags (inverse of StringIndexer)\n",
        "labels = label_indexer.labels\n",
        "mapping_expr = udf(lambda idx: labels[int(idx)], \"string\")\n",
        "final_predictions = predicted_labels.withColumn(\"PredictedTag\", mapping_expr(\"prediction\"))\n",
        "\n",
        "# 16. Save submission file\n",
        "final_predictions.select(\"Id\", \"PredictedTag\").coalesce(1).write.csv(\"submission_keywords\", header=True, mode=\"overwrite\")\n",
        "print(\"ðŸ“¤ Submission written to 'submission_keywords/'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXGsILhDvL/pO9uCof37g5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}