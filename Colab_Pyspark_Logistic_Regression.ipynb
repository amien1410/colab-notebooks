{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2sxEeLJcpeFPG6Vub3DXV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/colab-notebooks/blob/main/Colab_Pyspark_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0fb0c6c"
      },
      "source": [
        "## PySpark Logistic Regression Tutorial: Predicting Customer Churn 📊\n",
        "\n",
        "Welcome to this tutorial! We're going to dive into predicting customer churn using **PySpark** and **Logistic Regression**. 🚀\n",
        "\n",
        "Customer churn is a big deal for businesses. It means customers are leaving. By predicting which customers might churn, companies can take action to keep them. 🎯\n",
        "\n",
        "We'll be using a dataset from Kaggle. It contains information about consumers, including:\n",
        "\n",
        "*   **Names**: The name of the customer.\n",
        "*   **Age**: The customer's age.\n",
        "*   **Total\\_Purchase**: The total amount the customer has spent.\n",
        "*   **Account\\_Manager**: Whether the customer has an account manager (1 for yes, 0 for no).\n",
        "*   **Years**: The number of years the customer has been with the company.\n",
        "*   **Num\\_Sites**: The number of websites the customer uses.\n",
        "*   **Onboard\\_date**: The date the customer joined.\n",
        "*   **Location**: The customer's location.\n",
        "*   **Company**: The customer's company.\n",
        "*   **Churn**: Whether the customer churned (1 for yes, 0 for no). This is what we'll try to predict! 💪\n",
        "\n",
        "Get ready to learn how to use PySpark for this exciting machine learning task! 🎉"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvugIbGKV5OG",
        "outputId": "78537d57-4318-4ce4-b82f-e13687718c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Dataset URL: https://www.kaggle.com/datasets/brycepeakega/generalassemblywelcome5k\n",
            "License(s): unknown\n",
            "Downloading generalassemblywelcome5k.zip to /content\n",
            "  0% 0.00/5.99M [00:00<?, ?B/s]\n",
            "100% 5.99M/5.99M [00:00<00:00, 550MB/s]\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n",
        "!kaggle datasets download -d brycepeakega/generalassemblywelcome5k\n",
        "!unzip -q \"/content/generalassemblywelcome5k.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b27db5"
      },
      "source": [
        "In this cell, we're getting set up to work with some data. 🚀\n",
        "\n",
        "*   `from google.colab import drive`: This line imports a tool to connect Google Colab to your Google Drive. 🤝\n",
        "*   `drive.mount('/content/drive')`: This line actually makes the connection. Now, Colab can see and use files stored in your Google Drive. 📂\n",
        "*   `!pip install kaggle`: We're installing the Kaggle library. Kaggle is a platform for data science competitions and datasets. 🏆\n",
        "*   `import os`: This imports the 'os' module, which helps us interact with the computer's operating system. 💻\n",
        "*   `os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'`: This sets up a special location where Kaggle will look for your credentials. We're telling it to look inside your Google Drive. 🔑\n",
        "*   `!kaggle datasets download -d dansbecker/melbourne-housing-snapshot`: This command downloads a specific dataset from Kaggle about housing in Melbourne. 🏠\n",
        "*   `!unzip -q \"/content/melbourne-housing-snapshot.zip\"`: After downloading, the dataset is usually in a compressed format (like a zip file). This line unzips it so we can access the data inside. 📦"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e56150cd",
        "outputId": "222faa4b-725e-4aed-b5cd-57fed35711e5"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ChurnPredictionLogisticRegression\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load data\n",
        "df = spark.read.csv(\"/content/customer_churn.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Print schema and preview data\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Onboard_date: timestamp (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
            "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
            "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
            "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n",
            "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49087e12"
      },
      "source": [
        "Let's break down the code you just ran: 👇\n",
        "\n",
        "*   `from pyspark.sql import SparkSession`: This line imports the necessary class to create a Spark session. Think of a Spark session as your main entry point to using Spark's functionality. 🚪\n",
        "*   `from pyspark.ml.feature import StringIndexer, VectorAssembler`: We're importing tools for preparing our data. `StringIndexer` helps convert text categories into numbers, and `VectorAssembler` combines different columns into a single feature vector that machine learning models can use. 🛠️\n",
        "*   `from pyspark.ml.classification import LogisticRegression`: This imports the Logistic Regression algorithm from Spark's machine learning library. This is the model we'll use for prediction. 🧠\n",
        "*   `from pyspark.ml.evaluation import BinaryClassificationEvaluator`: This imports a tool to evaluate how well our binary classification model (like predicting churn, which is either yes or no) performs. ✅\n",
        "*   `from pyspark.ml import Pipeline`: This imports the `Pipeline` class, which allows us to chain multiple data processing and machine learning steps together. This makes our workflow organized and repeatable. 🏗️\n",
        "*   `spark = SparkSession.builder.appName(\"ChurnPredictionLogisticRegression\").getOrCreate()`: This is where we create or get an existing Spark session. We give it a name (\"ChurnPredictionLogisticRegression\") so we can identify it. ✨\n",
        "*   `df = spark.read.csv(\"customer_churn.csv\", header=True, inferSchema=True)`: This line loads our data from a CSV file named \"customer_churn.csv\" into a Spark DataFrame. `header=True` tells Spark that the first row is the header, and `inferSchema=True` tells Spark to automatically figure out the data types of each column. 📝\n",
        "*   `df.printSchema()`: This displays the structure of our DataFrame, showing the column names and their inferred data types. It's like looking at the blueprint of our data. 🗺️\n",
        "*   `df.show(5)`: This shows the first 5 rows of our DataFrame. It's a quick way to peek at the actual data. 👀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ed72100"
      },
      "source": [
        "# Assume the label column is named 'Churn' (Yes/No), convert it to numeric\n",
        "label_indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
        "\n",
        "# Select features (replace 'data' with your actual DataFrame name, which is 'df')\n",
        "feature_cols = [col for col in df.columns if col != 'Churn']\n",
        "\n",
        "# Handle categorical features (if any)\n",
        "# Example: Let's say 'Gender' and 'Contract' are categorical\n",
        "# categorical_cols = ['Gender', 'Contract']  # Replace with your categorical columns\n",
        "categorical_cols = [] # Based on the schema, there are no obvious categorical columns that need indexing\n",
        "\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_cols]\n",
        "\n",
        "# Replace categorical columns with indexed versions in features list\n",
        "indexed_feature_cols = [col + \"_indexed\" if col in categorical_cols else col for col in feature_cols]\n",
        "\n",
        "# Assemble features into a single vector\n",
        "assembler = VectorAssembler(inputCols=indexed_feature_cols, outputCol=\"features\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9906f5f2"
      },
      "source": [
        "Let's break down the data preprocessing steps: 👇\n",
        "\n",
        "*   `label_indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")`: This creates a `StringIndexer` to convert the 'Churn' column (our target variable) into a numerical format. Machine learning models typically work with numerical data. We're naming the new numerical column 'label'. 🔢\n",
        "*   `feature_cols = [col for col in df.columns if col != 'Churn']`: This line creates a list of all column names in our DataFrame *except* for the 'Churn' column. These will be our input features for the model. 📋\n",
        "*   `categorical_cols = []`: We initialize an empty list for categorical columns. Based on the schema we saw earlier, there aren't any columns that immediately appear to be categorical and require indexing for this specific dataset. If your dataset had columns like 'Gender' or 'Contract' with text values, you would list them here. 📝\n",
        "*   `indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_cols]`: If we had categorical columns listed, this would create a `StringIndexer` for each of them to convert their text values into numerical indices. The new indexed columns would have \"_indexed\" added to their original name. 🔤➡️🔢\n",
        "*   `indexed_feature_cols = [col + \"_indexed\" if col in categorical_cols else col for col in feature_cols]`: This updates our list of feature columns. If a column was identified as categorical, we use its new indexed name (e.g., 'Gender\\_indexed'); otherwise, we use the original column name. This ensures our feature list contains the numerical representations of any categorical features. 🔄\n",
        "*   `assembler = VectorAssembler(inputCols=indexed_feature_cols, outputCol=\"features\")`: This creates a `VectorAssembler`. This tool takes all the specified input columns (our `indexed_feature_cols`) and combines them into a single vector column named 'features'. Logistic Regression in PySpark expects the input features to be in this vector format. 💪"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c0910d1"
      },
      "source": [
        "# Split data\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db63f242"
      },
      "source": [
        "Let's break down the data splitting process: 👇\n",
        "\n",
        "*   `train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)`: This line takes our `df` DataFrame and randomly splits it into two parts: `train_data` (80% of the data) and `test_data` (20% of the data). The `seed=42` ensures that the split is the same every time you run the code, which is good for reproducibility. 🎲\n",
        "\n",
        "We now have two separate DataFrames: one for training our model and one for testing its performance. 🛠️✅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d92217d8"
      },
      "source": [
        "# Build Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='label')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6152de6"
      },
      "source": [
        "Let's break down this step: 👇\n",
        "\n",
        "*   `lr = LogisticRegression(featuresCol='features', labelCol='label')`: This line creates an instance of the `LogisticRegression` model. We're telling it to use the column named 'features' as the input features and the column named 'label' as the target variable (what we want to predict). 🎯\n",
        "\n",
        "We've now initialized our Logistic Regression model, ready to be trained! 💪"
      ]
    }
  ]
}