{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAQSDrkPE3SRmFCRPA5gY1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/colab-notebooks/blob/main/Colab_Pyspark_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0fb0c6c"
      },
      "source": [
        "## PySpark Logistic Regression Tutorial: Predicting Customer Churn 📊\n",
        "\n",
        "Welcome to this tutorial! We're going to dive into predicting customer churn using **PySpark** and **Logistic Regression**. 🚀\n",
        "\n",
        "Customer churn is a big deal for businesses. It means customers are leaving. By predicting which customers might churn, companies can take action to keep them. 🎯\n",
        "\n",
        "We'll be using a dataset from Kaggle. It contains information about consumers, including:\n",
        "\n",
        "*   **Names**: The name of the customer.\n",
        "*   **Age**: The customer's age.\n",
        "*   **Total\\_Purchase**: The total amount the customer has spent.\n",
        "*   **Account\\_Manager**: Whether the customer has an account manager (1 for yes, 0 for no).\n",
        "*   **Years**: The number of years the customer has been with the company.\n",
        "*   **Num\\_Sites**: The number of websites the customer uses.\n",
        "*   **Onboard\\_date**: The date the customer joined.\n",
        "*   **Location**: The customer's location.\n",
        "*   **Company**: The customer's company.\n",
        "*   **Churn**: Whether the customer churned (1 for yes, 0 for no). This is what we'll try to predict! 💪\n",
        "\n",
        "Get ready to learn how to use PySpark for this exciting machine learning task! 🎉"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvugIbGKV5OG",
        "outputId": "78537d57-4318-4ce4-b82f-e13687718c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Dataset URL: https://www.kaggle.com/datasets/brycepeakega/generalassemblywelcome5k\n",
            "License(s): unknown\n",
            "Downloading generalassemblywelcome5k.zip to /content\n",
            "  0% 0.00/5.99M [00:00<?, ?B/s]\n",
            "100% 5.99M/5.99M [00:00<00:00, 550MB/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Download the dataset from Kaggle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install kaggle\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'\n",
        "!kaggle datasets download -d brycepeakega/generalassemblywelcome5k\n",
        "!unzip -q \"/content/generalassemblywelcome5k.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b27db5"
      },
      "source": [
        "In this cell, we're getting set up to work with some data. 🚀\n",
        "\n",
        "*   `from google.colab import drive`: This line imports a tool to connect Google Colab to your Google Drive. 🤝\n",
        "*   `drive.mount('/content/drive')`: This line actually makes the connection. Now, Colab can see and use files stored in your Google Drive. 📂\n",
        "*   `!pip install kaggle`: We're installing the Kaggle library. Kaggle is a platform for data science competitions and datasets. 🏆\n",
        "*   `import os`: This imports the 'os' module, which helps us interact with the computer's operating system. 💻\n",
        "*   `os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'`: This sets up a special location where Kaggle will look for your credentials. We're telling it to look inside your Google Drive. 🔑\n",
        "*   `!kaggle datasets download -d dansbecker/melbourne-housing-snapshot`: This command downloads a specific dataset from Kaggle about housing in Melbourne. 🏠\n",
        "*   `!unzip -q \"/content/melbourne-housing-snapshot.zip\"`: After downloading, the dataset is usually in a compressed format (like a zip file). This line unzips it so we can access the data inside. 📦"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e56150cd",
        "outputId": "222faa4b-725e-4aed-b5cd-57fed35711e5"
      },
      "source": [
        "#@title Import the libraries, start Spark Session and load the dataset\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ChurnPredictionLogisticRegression\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load data\n",
        "df = spark.read.csv(\"/content/customer_churn.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Print schema and preview data\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Names: string (nullable = true)\n",
            " |-- Age: double (nullable = true)\n",
            " |-- Total_Purchase: double (nullable = true)\n",
            " |-- Account_Manager: integer (nullable = true)\n",
            " |-- Years: double (nullable = true)\n",
            " |-- Num_Sites: double (nullable = true)\n",
            " |-- Onboard_date: timestamp (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- Company: string (nullable = true)\n",
            " |-- Churn: integer (nullable = true)\n",
            "\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|       Onboard_date|            Location|             Company|Churn|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|2013-08-30 07:00:40|10265 Elizabeth M...|          Harvey LLC|    1|\n",
            "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|2013-08-13 00:38:46|6157 Frank Garden...|          Wilson PLC|    1|\n",
            "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|2016-06-29 06:20:07|1331 Keith Court ...|Miller, Johnson a...|    1|\n",
            "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|2014-04-22 12:43:12|13120 Daniel Moun...|           Smith Inc|    1|\n",
            "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|2016-01-19 15:31:15|765 Tricia Row Ka...|          Love-Jones|    1|\n",
            "+----------------+----+--------------+---------------+-----+---------+-------------------+--------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49087e12"
      },
      "source": [
        "Let's break down the code you just ran: 👇\n",
        "\n",
        "*   `from pyspark.sql import SparkSession`: This line imports the necessary class to create a Spark session. Think of a Spark session as your main entry point to using Spark's functionality. 🚪\n",
        "*   `from pyspark.ml.feature import StringIndexer, VectorAssembler`: We're importing tools for preparing our data. `StringIndexer` helps convert text categories into numbers, and `VectorAssembler` combines different columns into a single feature vector that machine learning models can use. 🛠️\n",
        "*   `from pyspark.ml.classification import LogisticRegression`: This imports the Logistic Regression algorithm from Spark's machine learning library. This is the model we'll use for prediction. 🧠\n",
        "*   `from pyspark.ml.evaluation import BinaryClassificationEvaluator`: This imports a tool to evaluate how well our binary classification model (like predicting churn, which is either yes or no) performs. ✅\n",
        "*   `from pyspark.ml import Pipeline`: This imports the `Pipeline` class, which allows us to chain multiple data processing and machine learning steps together. This makes our workflow organized and repeatable. 🏗️\n",
        "*   `spark = SparkSession.builder.appName(\"ChurnPredictionLogisticRegression\").getOrCreate()`: This is where we create or get an existing Spark session. We give it a name (\"ChurnPredictionLogisticRegression\") so we can identify it. ✨\n",
        "*   `df = spark.read.csv(\"customer_churn.csv\", header=True, inferSchema=True)`: This line loads our data from a CSV file named \"customer_churn.csv\" into a Spark DataFrame. `header=True` tells Spark that the first row is the header, and `inferSchema=True` tells Spark to automatically figure out the data types of each column. 📝\n",
        "*   `df.printSchema()`: This displays the structure of our DataFrame, showing the column names and their inferred data types. It's like looking at the blueprint of our data. 🗺️\n",
        "*   `df.show(5)`: This shows the first 5 rows of our DataFrame. It's a quick way to peek at the actual data. 👀"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ed72100"
      },
      "source": [
        "#@title Select features and target, assemble features into vector\n",
        "\n",
        "# Select features, excluding non-numeric and non-categorical columns that don't need indexing\n",
        "feature_cols = ['Age', 'Total_Purchase', 'Account_Manager', 'Years', 'Num_Sites']\n",
        "\n",
        "# Optional: Index categorical 'Location' (if you think it adds signal)\n",
        "location_indexer = StringIndexer(inputCol='Location', outputCol='Location_indexed', handleInvalid='keep')\n",
        "\n",
        "# Add the indexed location to feature columns\n",
        "feature_cols.append('Location_indexed')\n",
        "\n",
        "# Step 5: Assemble features into vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9906f5f2"
      },
      "source": [
        "Let's break down the data preprocessing steps: 👇\n",
        "\n",
        "*   `label_indexer = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")`: This creates a `StringIndexer` to convert the 'Churn' column (our target variable) into a numerical format. Machine learning models typically work with numerical data. We're naming the new numerical column 'label'. 🔢\n",
        "*   `feature_cols = [col for col in df.columns if col != 'Churn']`: This line creates a list of all column names in our DataFrame *except* for the 'Churn' column. These will be our input features for the model. 📋\n",
        "*   `categorical_cols = []`: We initialize an empty list for categorical columns. Based on the schema we saw earlier, there aren't any columns that immediately appear to be categorical and require indexing for this specific dataset. If your dataset had columns like 'Gender' or 'Contract' with text values, you would list them here. 📝\n",
        "*   `indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_cols]`: If we had categorical columns listed, this would create a `StringIndexer` for each of them to convert their text values into numerical indices. The new indexed columns would have \"_indexed\" added to their original name. 🔤➡️🔢\n",
        "*   `indexed_feature_cols = [col + \"_indexed\" if col in categorical_cols else col for col in feature_cols]`: This updates our list of feature columns. If a column was identified as categorical, we use its new indexed name (e.g., 'Gender\\_indexed'); otherwise, we use the original column name. This ensures our feature list contains the numerical representations of any categorical features. 🔄\n",
        "*   `assembler = VectorAssembler(inputCols=indexed_feature_cols, outputCol=\"features\")`: This creates a `VectorAssembler`. This tool takes all the specified input columns (our `indexed_feature_cols`) and combines them into a single vector column named 'features'. Logistic Regression in PySpark expects the input features to be in this vector format. 💪"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c0910d1"
      },
      "source": [
        "#@title Split the dataset into training and testing data\n",
        "\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db63f242"
      },
      "source": [
        "Let's break down the data splitting process: 👇\n",
        "\n",
        "*   `train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)`: This line takes our `df` DataFrame and randomly splits it into two parts: `train_data` (80% of the data) and `test_data` (20% of the data). The `seed=42` ensures that the split is the same every time you run the code, which is good for reproducibility. 🎲\n",
        "\n",
        "We now have two separate DataFrames: one for training our model and one for testing its performance. 🛠️✅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d92217d8"
      },
      "source": [
        "#@title Build Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol='features', labelCol='Churn')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6152de6"
      },
      "source": [
        "Let's break down this step: 👇\n",
        "\n",
        "*   `lr = LogisticRegression(featuresCol='features', labelCol='label')`: This line creates an instance of the `LogisticRegression` model. We're telling it to use the column named 'features' as the input features and the column named 'label' as the target variable (what we want to predict). 🎯\n",
        "\n",
        "We've now initialized our Logistic Regression model, ready to be trained! 💪"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5698df0"
      },
      "source": [
        "#@title Create ML pipeline\n",
        "pipeline = Pipeline(stages=[location_indexer, assembler, lr])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a861440c"
      },
      "source": [
        "Let's break down the pipeline creation: 👇\n",
        "\n",
        "*   `pipeline = Pipeline(stages=indexers + [label_indexer, assembler, lr])`: This line creates a Spark ML `Pipeline`. A pipeline allows us to combine multiple `Estimators` and `Transformers` into a single workflow. 🔄\n",
        "    *   `indexers`: This includes any `StringIndexer` stages we created for categorical features. If `categorical_cols` was empty, this list is empty.\n",
        "    *   `label_indexer`: This is the `StringIndexer` for our target variable, 'Churn'.\n",
        "    *   `assembler`: This is the `VectorAssembler` that combines our features into a single vector column.\n",
        "    *   `lr`: This is our `LogisticRegression` model.\n",
        "\n",
        "The pipeline will execute these stages in order: first, it will index categorical features (if any), then index the label column, then assemble the features, and finally apply the logistic regression model. This ensures our data is correctly transformed before being fed into the model. ✨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd5284dd"
      },
      "source": [
        "#@title Train the model\n",
        "model = pipeline.fit(train_data)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dac89f07"
      },
      "source": [
        "Let's break down the model training step: 👇\n",
        "\n",
        "*   `model = pipeline.fit(train_data)`: This line trains the entire pipeline using the `train_data` DataFrame. When you call `.fit()` on a `Pipeline`, it sequentially runs the `.fit()` method on each `Estimator` (like `StringIndexer` and `LogisticRegression`) and the `.transform()` method on each `Transformer` (like `VectorAssembler`) within the pipeline. The result is a `PipelineModel`, which is a trained pipeline. 🚀\n",
        "\n",
        "The `model` variable now holds our trained pipeline, ready to make predictions! 💪"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e90ccb80"
      },
      "source": [
        "#@title Predict on test data\n",
        "predictions = model.transform(test_data)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b21c4ad"
      },
      "source": [
        "Let's break down the prediction step: 👇\n",
        "\n",
        "*   `predictions = model.transform(test_data)`: This line uses our trained `model` (which is a `PipelineModel`) to make predictions on the `test_data` DataFrame. The `.transform()` method applies all the stages in the trained pipeline (including feature assembly and the trained logistic regression model) to the new data. 🚀\n",
        "\n",
        "The result is a new DataFrame called `predictions` that includes the original columns from `test_data` plus additional columns generated by the pipeline, such as the predicted churn label and the raw prediction scores. ✨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57fd6756",
        "outputId": "06f26f1d-dd04-428d-8053-d330552b64b2"
      },
      "source": [
        "#@title Evaluate the model using AUC\n",
        "evaluator = BinaryClassificationEvaluator(labelCol='Churn', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
        "roc_auc = evaluator.evaluate(predictions)\n",
        "print(f\"ROC AUC Score: {roc_auc:.4f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC Score: 0.8795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33893f05"
      },
      "source": [
        "Let's break down the model evaluation step: 👇\n",
        "\n",
        "*   `evaluator = BinaryClassificationEvaluator(labelCol='Churn', rawPredictionCol='rawPrediction', metricName='areaUnderROC')`: This line creates a `BinaryClassificationEvaluator`. We configure it to use the 'Churn' column as the true labels, the 'rawPrediction' column (generated by the logistic regression model) for the prediction scores, and specify that we want to calculate the 'areaUnderROC' metric (AUC). The AUC measures the ability of the classifier to distinguish between positive and negative classes. A higher AUC indicates better performance. ✅\n",
        "*   `roc_auc = evaluator.evaluate(predictions)`: This line runs the evaluator on our `predictions` DataFrame to calculate the ROC AUC score. 📊\n",
        "*   `print(f\"ROC AUC Score: {roc_auc:.4f}\")`: This line prints the calculated ROC AUC score, formatted to four decimal places, so we can see how well our model performed. 🎯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a613e6"
      },
      "source": [
        "Let's break down the ROC AUC Score: 👇\n",
        "\n",
        "*   **ROC AUC Score: 0.8795** ✨: This score, standing at **0.8795**, is a key metric for evaluating binary classification models like the one we built to predict customer churn.\n",
        "*   **What it means**: The Area Under the Receiver Operating Characteristic Curve (ROC AUC) essentially measures the model's ability to distinguish between the two classes (churned vs. not churned).\n",
        "*   **Interpretation**: A score of 0.5 suggests the model performs no better than random chance, while a score of 1.0 indicates a perfect model. Our score of **0.8795** is quite good! It means our model has a strong ability to differentiate between customers who are likely to churn and those who are not. 💪\n",
        "*   **In simpler terms**: The model is reasonably accurate at identifying potential churners, which is valuable for a business wanting to retain customers. 🎯"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29da1c8b",
        "outputId": "5e56dcce-9f5c-43b7-84b8-68d295ea1648"
      },
      "source": [
        "#@title Show predictions\n",
        "predictions.select(\"Churn\", \"prediction\", \"probability\").show(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+--------------------+\n",
            "|Churn|prediction|         probability|\n",
            "+-----+----------+--------------------+\n",
            "|    0|       0.0|[0.95450919377362...|\n",
            "|    0|       0.0|[0.87345478839347...|\n",
            "|    0|       0.0|[0.96324976241585...|\n",
            "|    0|       0.0|[0.90264082322965...|\n",
            "|    0|       0.0|[0.96275299577163...|\n",
            "|    0|       0.0|[0.90119262811257...|\n",
            "|    0|       0.0|[0.98999787282630...|\n",
            "|    0|       0.0|[0.97989748156890...|\n",
            "|    0|       0.0|[0.86721806205680...|\n",
            "|    0|       0.0|[0.55978175465318...|\n",
            "+-----+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff7bbb89"
      },
      "source": [
        "Let's break down the prediction display: 👇\n",
        "\n",
        "*   `predictions.select(\"Churn\", \"prediction\", \"probability\")`: This selects the original 'Churn' column (the actual value), the 'prediction' column (the model's predicted churn value, either 0 or 1), and the 'probability' column (the probability distribution of the prediction, showing the likelihood of the customer being in each class). 📊\n",
        "*   `.show(10)`: This displays the first 10 rows of the selected columns, allowing us to quickly inspect the model's predictions and compare them to the actual values. 📋\n",
        "\n",
        "This gives us a glimpse into how our model is performing on individual instances in the test set. ✨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f5858de"
      },
      "source": [
        "## Wrapping Up: What We've Learned and What's Next! 🎉\n",
        "\n",
        "We've successfully built a customer churn prediction model using PySpark's Logistic Regression! Here's a quick recap of what we covered:\n",
        "\n",
        "*   **Spark Session**: We learned how to start a Spark session, which is our gateway to using PySpark. 🚪\n",
        "*   **Data Loading and Exploration**: We loaded a CSV dataset into a Spark DataFrame and explored its schema and initial rows. 📊\n",
        "*   **Data Preprocessing**: We prepared our data by handling the label column and assembling features into a vector format suitable for machine learning models. 🛠️\n",
        "*   **Model Building**: We initialized a Logistic Regression model. 🧠\n",
        "*   **Pipeline Creation**: We built a pipeline to chain our preprocessing and modeling steps together. 🏗️\n",
        "*   **Model Training**: We trained the pipeline model on our training data. 🚀\n",
        "*   **Prediction**: We used the trained model to make predictions on the test data. ✅\n",
        "*   **Evaluation**: We evaluated the model's performance using the ROC AUC score. 🎯\n",
        "*   **Prediction Inspection**: We looked at some individual predictions to see the actual vs. predicted churn and associated probabilities. 👀\n",
        "\n",
        "### What's Next? 🤔\n",
        "\n",
        "This tutorial provides a solid foundation. Here are some exciting next steps you could explore:\n",
        "\n",
        "*   **Feature Engineering**: Create new features from existing ones (e.g., customer engagement metrics, duration since last activity).\n",
        "*   **Handling Categorical Features**: Properly handle categorical features like 'Location' and 'Company' using techniques like `StringIndexer` and `OneHotEncoderEstimator` if you believe they contain valuable information for predicting churn.\n",
        "*   **Hyperparameter Tuning**: Experiment with different hyperparameters for the Logistic Regression model to potentially improve performance (e.g., regularization parameter `regParam`, elastic net mixing `elasticNetParam`).\n",
        "*   **Other Algorithms**: Try other PySpark ML classification algorithms like Decision Trees, Random Forests, or Gradient Boosted Trees to see if they yield better results.\n",
        "*   **Cross-Validation**: Implement cross-validation for more robust model evaluation.\n",
        "*   **Model Interpretation**: Explore techniques to understand which features are most important for the model's predictions.\n",
        "*   **Saving and Loading**: Learn how to save your trained model and load it later for making new predictions.\n",
        "\n",
        "### Where Can This Be Applied? 💡\n",
        "\n",
        "Customer churn prediction is valuable across many industries:\n",
        "\n",
        "*   **Telecommunications**: Predicting which customers are likely to switch providers.\n",
        "*   **Subscription Services (Streaming, SaaS)**: Identifying users at risk of canceling their subscriptions.\n",
        "*   **Banking and Finance**: Predicting customers who might close accounts.\n",
        "*   **E-commerce**: Predicting customers who might stop making purchases.\n",
        "\n",
        "### Areas for Improvement 🚧\n",
        "\n",
        "While our model performed well, there are always areas to improve:\n",
        "\n",
        "*   **Data Quality**: Ensure the data is clean and accurate. Missing values or inconsistencies can impact model performance.\n",
        "*   **Feature Importance**: Understand which features are driving the predictions. This can help in business decision-making.\n",
        "*   **Class Imbalance**: If churned customers are a small percentage of the total, the dataset might be imbalanced. Techniques like oversampling or undersampling could be explored.\n",
        "*   **Deployment**: For real-world applications, the model needs to be deployed to make predictions on new data.\n",
        "\n",
        "Keep experimenting and building! Happy coding! 😊"
      ]
    }
  ]
}