{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3JYSwbkt19HgY6O7t9lYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amien1410/colab-notebooks/blob/main/Colab_Pyspark_Utility_Functions_and_Visualizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HazlZ5np-C4x",
        "outputId": "bfc8ad05-55af-4710-fed7-2f460a02915b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "--2025-06-11 03:16:15--  https://raw.githubusercontent.com/Apress/applied-data-science-using-pyspark/refs/heads/main/Ch02/Chapter2_Data/movie_data_part1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29227553 (28M) [text/plain]\n",
            "Saving to: ‘movies.csv’\n",
            "\n",
            "movies.csv          100%[===================>]  27.87M  25.2MB/s    in 1.1s    \n",
            "\n",
            "2025-06-11 03:16:17 (25.2 MB/s) - ‘movies.csv’ saved [29227553/29227553]\n",
            "\n",
            "+---------------------+------+-----+-----------------+--------------------+--------------------+----------+--------------------+--------------------+------------+-------+-------+--------+--------------------+--------------------+------------+\n",
            "|belongs_to_collection|budget|   id|original_language|      original_title|            overview|popularity|production_companies|production_countries|release_date|revenue|runtime|  status|             tagline|               title|vote_average|\n",
            "+---------------------+------+-----+-----------------+--------------------+--------------------+----------+--------------------+--------------------+------------+-------+-------+--------+--------------------+--------------------+------------+\n",
            "|                 NULL|     0|43000|               fr|  Le Caporal épinglé|The story serves ...|     2.503|[{'id': 16059, 'l...|[{'iso_3166_1': '...|  1962-05-23|      0|     90|Released|                NULL|The Elusive Corporal|         5.9|\n",
            "|                 NULL|     0|43001|               fr|Cybèle ou les dim...|The tragic story ...|      5.51|[{'id': 7808, 'lo...|[{'iso_3166_1': '...|  1962-11-12|      0|    110|Released|                NULL|  Sundays and Cybele|         7.4|\n",
            "|                 NULL|     0|43002|               en|Lonely Are the Brave|A fiercely indepe...|      5.62|[{'id': 3810, 'lo...|[{'iso_3166_1': '...|  1962-05-24|      0|    107|Released|Life can never ca...|Lonely Are the Brave|         7.5|\n",
            "|                 NULL|     0|43003|               fr|Vérités et Mensonges|Documents the liv...|     7.159|[{'id': 36547, 'l...|[{'iso_3166_1': '...|  1975-03-12|      0|     89|Released|                NULL|          F for Fake|         7.5|\n",
            "|                 NULL|500000|43004|               en|Long Day's Journe...|The film follows ...|     3.988|[{'id': 77598, 'l...|[{'iso_3166_1': '...|  1962-10-09|      0|    174|Released|PRIDE...POWER...P...|Long Day's Journe...|         6.8|\n",
            "|                 NULL|     0|43006|               en|           My Geisha|\"A director's (Yv...|     3.194|[{'id': 82939, 'l...|[{'iso_3166_1': '...|  1962-03-09|      0|    119|Released|                NULL|           My Geisha|         6.1|\n",
            "|                 NULL|     0|43007|               en|Period of Adjustment|George Haverstick...|     2.689|[{'id': 13359, 'l...|[{'iso_3166_1': '...|  1962-10-31|      0|    112|Released|The agonizing pau...|Period of Adjustment|         6.4|\n",
            "|                 NULL|     0|43008|               en|    The Hanging Tree|Character study o...|     6.537|[{'id': 14248, 'l...|[{'iso_3166_1': '...|  1959-03-13|      0|    107|Released|From The Prize No...|    The Hanging Tree|         6.6|\n",
            "|                 NULL|     0|43010|               de|Sherlock Holmes u...|Sherlock Holmes a...|     4.297|[{'id': 1787, 'lo...|[{'iso_3166_1': '...|  1962-01-01|      0|     87|Released|                NULL|Sherlock Holmes a...|         5.5|\n",
            "|                 NULL|     0|43011|               en|  Sodom and Gomorrah|Sex, torture and ...|     4.417|[{'id': 8894, 'lo...|[{'iso_3166_1': '...|  1962-01-01|      0|    154|Released|The cities that m...|  Sodom and Gomorrah|         5.4|\n",
            "+---------------------+------+-----+-----------------+--------------------+--------------------+----------+--------------------+--------------------+------------+-------+-------+--------+--------------------+--------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# install pyspark\n",
        "!pip install pyspark\n",
        "\n",
        "# initiate pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "# download sample\n",
        "!wget -O movies.csv \"https://raw.githubusercontent.com/Apress/applied-data-science-using-pyspark/refs/heads/main/Ch02/Chapter2_Data/movie_data_part1.csv\"\n",
        "df = spark.read.csv(\"movies.csv\", header=True, sep='|', inferSchema=False)\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Casting Multiple Columns\n",
        "\n",
        "#Importing necessary libraries\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "#Identifying and assigning lists of variables\n",
        "int_vars=['id']\n",
        "float_vars=['budget', 'popularity', 'revenue']\n",
        "date_vars=['release_date']\n",
        "\n",
        "#Converting integer variables\n",
        "for column in int_vars:\n",
        "  df = df.withColumn(column,df[column].cast(IntegerType()))\n",
        "\n",
        "# Converting float variables\n",
        "for column in float_vars:\n",
        "  df=df.withColumn(column,df[column].cast(FloatType()))\n",
        "\n",
        "# Converting float variables\n",
        "for column in date_vars:\n",
        "  df=df.withColumn(column,df[column].cast(DateType()))\n",
        "\n",
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7BM0P10A5k0",
        "outputId": "754c8018-d1bb-436a-b41b-e1b828177f01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('belongs_to_collection', 'string'),\n",
              " ('budget', 'float'),\n",
              " ('id', 'int'),\n",
              " ('original_language', 'string'),\n",
              " ('original_title', 'string'),\n",
              " ('overview', 'string'),\n",
              " ('popularity', 'float'),\n",
              " ('production_companies', 'string'),\n",
              " ('production_countries', 'string'),\n",
              " ('release_date', 'date'),\n",
              " ('revenue', 'float'),\n",
              " ('runtime', 'string'),\n",
              " ('status', 'string'),\n",
              " ('tagline', 'string'),\n",
              " ('title', 'string'),\n",
              " ('vote_average', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using when function\n",
        "df_with_newcols = df.select('id', 'budget','popularity').\\\n",
        "withColumn('budget_cat', when(df['budget']<10000000,'Small').when(df['budget']<100000000,'Medium').otherwise('Big')).\\\n",
        "withColumn('ratings', when(df['popularity']<3,'Low').\n",
        "when(df['popularity']<5,'Mid').otherwise('High'))\n",
        "\n",
        "df_with_newcols.show(15, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Add-SjH-iXH",
        "outputId": "eaa42e87-3148-4e95-9004-f616be127aaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+----------+----------+-------+\n",
            "|id   |budget   |popularity|budget_cat|ratings|\n",
            "+-----+---------+----------+----------+-------+\n",
            "|43000|0.0      |2.503     |Small     |Low    |\n",
            "|43001|0.0      |5.51      |Small     |High   |\n",
            "|43002|0.0      |5.62      |Small     |High   |\n",
            "|43003|0.0      |7.159     |Small     |High   |\n",
            "|43004|500000.0 |3.988     |Small     |Mid    |\n",
            "|43006|0.0      |3.194     |Small     |Mid    |\n",
            "|43007|0.0      |2.689     |Small     |Low    |\n",
            "|43008|0.0      |6.537     |Small     |High   |\n",
            "|43010|0.0      |4.297     |Small     |Mid    |\n",
            "|43011|0.0      |4.417     |Small     |Mid    |\n",
            "|43012|7000000.0|4.722     |Small     |Mid    |\n",
            "|43013|0.0      |2.543     |Small     |Low    |\n",
            "|43014|0.0      |4.303     |Small     |Mid    |\n",
            "|43015|0.0      |3.493     |Small     |Mid    |\n",
            "|43016|0.0      |2.851     |Small     |Low    |\n",
            "+-----+---------+----------+----------+-------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating two variables\n",
        "df_with_newcols = df_with_newcols.withColumn('BudgetRating_Category',concat(df_with_newcols.budget_cat,df_with_newcols.ratings))\n",
        "\n",
        "# Changing the new variable to lowercase\n",
        "df_with_newcols = df_with_newcols.withColumn('BudgetRating_Category',trim(lower(df_with_newcols.BudgetRating_Category)))\n",
        "df_with_newcols.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTjc41ctCCRe",
        "outputId": "4123c5b3-76b5-4237-8bcf-8764811db86b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+----------+----------+-------+---------------------+\n",
            "|   id|   budget|popularity|budget_cat|ratings|BudgetRating_Category|\n",
            "+-----+---------+----------+----------+-------+---------------------+\n",
            "|43000|      0.0|     2.503|     Small|    Low|             smalllow|\n",
            "|43001|      0.0|      5.51|     Small|   High|            smallhigh|\n",
            "|43002|      0.0|      5.62|     Small|   High|            smallhigh|\n",
            "|43003|      0.0|     7.159|     Small|   High|            smallhigh|\n",
            "|43004| 500000.0|     3.988|     Small|    Mid|             smallmid|\n",
            "|43006|      0.0|     3.194|     Small|    Mid|             smallmid|\n",
            "|43007|      0.0|     2.689|     Small|    Low|             smalllow|\n",
            "|43008|      0.0|     6.537|     Small|   High|            smallhigh|\n",
            "|43010|      0.0|     4.297|     Small|    Mid|             smallmid|\n",
            "|43011|      0.0|     4.417|     Small|    Mid|             smallmid|\n",
            "|43012|7000000.0|     4.722|     Small|    Mid|             smallmid|\n",
            "|43013|      0.0|     2.543|     Small|    Low|             smalllow|\n",
            "|43014|      0.0|     4.303|     Small|    Mid|             smallmid|\n",
            "|43015|      0.0|     3.493|     Small|    Mid|             smallmid|\n",
            "|43016|      0.0|     2.851|     Small|    Low|             smalllow|\n",
            "|43017|      0.0|     4.047|     Small|    Mid|             smallmid|\n",
            "|43018|      0.0|     2.661|     Small|    Low|             smalllow|\n",
            "|43019|      0.0|     3.225|     Small|    Mid|             smallmid|\n",
            "|43020|      0.0|      5.72|     Small|   High|            smallhigh|\n",
            "|43021|      0.0|     3.292|     Small|    Mid|             smallmid|\n",
            "+-----+---------+----------+----------+-------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Registering Dataframes\n",
        "\n",
        "# Registering temporary table\n",
        "df_with_newcols.registerTempTable('temp_data')\n",
        "\n",
        "# Applying the function to show the results\n",
        "spark.sql('select ratings, count(ratings) from temp_data group by ratings').show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGXLGjhsCe8L",
        "outputId": "10f66f8f-2328-4cdb-afa4-70b53d9ccb9d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py:329: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+\n",
            "|ratings|count(ratings)|\n",
            "+-------+--------------+\n",
            "|High   |16856         |\n",
            "|Low    |14865         |\n",
            "|Mid    |12277         |\n",
            "+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Window Functions\n",
        "\n",
        "# Importing the window functions\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "# Step 1: Filtering the missing values\n",
        "df_with_newcols=df_with_newcols.filter( (df_with_newcols['popularity'].isNotNull()) & (~isnan(df_with_newcols['popularity'])) )\n",
        "\n",
        "# Step 2: Applying the window functions for calculating deciles\n",
        "df_with_newcols = df_with_newcols.select(\"id\",\"budget\",\"popularity\", ntile(10).over(Window.partitionBy().orderBy(df_with_newcols['popularity'].desc())).alias(\"decile_rank\"))\n",
        "\n",
        "# Step 3:Dispalying the values\n",
        "df_with_newcols.groupby(\"decile_rank\").agg(min('popularity').alias('min_popularity'),max('popularity').alias('max_popularity'),count('popularity')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr0Ox48wEmWK",
        "outputId": "cea66bef-f38d-4703-e1ea-9af76ea31754"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+--------------+-----------------+\n",
            "|decile_rank|min_popularity|max_popularity|count(popularity)|\n",
            "+-----------+--------------+--------------+-----------------+\n",
            "|          1|        10.185|         180.0|             4379|\n",
            "|          2|         7.481|        10.182|             4379|\n",
            "|          3|         5.841|         7.481|             4379|\n",
            "|          4|         4.823|         5.841|             4378|\n",
            "|          5|         4.054|         4.822|             4378|\n",
            "|          6|         3.383|         4.054|             4378|\n",
            "|          7|         2.747|         3.383|             4378|\n",
            "|          8|         2.075|         2.747|             4378|\n",
            "|          9|         1.389|         2.075|             4378|\n",
            "|         10|           0.6|         1.389|             4378|\n",
            "+-----------+--------------+--------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title FInd the second most popular movie in the year 1970\n",
        "\n",
        "# Step 1: Import the window functions\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "# Step 2: Select the required subset of columns\n",
        "df_second_best = df.select('id', 'popularity', 'release_date')\n",
        "\n",
        "# Step 3: Create the year column from release date\n",
        "df_second_best = df_second_best.withColumn('release_year',year('release_date')).drop('release_date')\n",
        "\n",
        "# Step 4: Define partition function\n",
        "year_window = Window.partitionBy(df_second_best['release_year']).orderBy(df_with_newcols['popularity'].desc())\n",
        "\n",
        "# Step 5: Apply partition function\n",
        "df_second_best = df_second_best.select('id', 'popularity', 'release_year',rank().over(year_window).alias(\"rank\"))\n",
        "\n",
        "# Step 6: Find the second best rating for the year 1970\n",
        "df_second_best.filter((df_second_best['release_year'] == 1970) & (df_second_best['rank']==2)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkiT9T4wGce1",
        "outputId": "82ea348f-263f-4992-8c15-ac79fd3234a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+------------+----+\n",
            "|   id|popularity|release_year|rank|\n",
            "+-----+----------+------------+----+\n",
            "|11202|    14.029|        1970|   2|\n",
            "+-----+----------+------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title what is the difference between the revenue of the highest-grossing film of the year and other films within that year?\n",
        "\n",
        "from pyspark.sql.functions import col, to_date, year, max\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Convert release_date and extract year\n",
        "df_revenue = df.select(\n",
        "    'id',\n",
        "    'revenue',\n",
        "    to_date('release_date', 'yyyy-MM-dd').alias('release_date')\n",
        ").withColumn('release_year', year('release_date'))\n",
        "\n",
        "# Filter out rows with nulls in critical columns\n",
        "df_revenue = df_revenue.filter(col('id').isNotNull() & col('revenue').isNotNull() & col('release_year').isNotNull())\n",
        "\n",
        "# Define window\n",
        "windowRev = Window.partitionBy('release_year')\\\n",
        "                  .orderBy(col('revenue').desc())\\\n",
        "                  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "# Add revenue difference\n",
        "df_revenue = df_revenue.withColumn(\n",
        "    'revenue_difference',\n",
        "    max(col('revenue')).over(windowRev) - col('revenue')\n",
        ")\n",
        "\n",
        "# Show final result\n",
        "df_revenue.select('id', 'revenue', 'release_year', 'revenue_difference').show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xZf7UXVHBWE",
        "outputId": "5b20cd2d-09fc-4600-d691-96a3c14196c6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+------------+------------------+\n",
            "|id   |revenue  |release_year|revenue_difference|\n",
            "+-----+---------+------------+------------------+\n",
            "|70512|0.0      |1911        |0.0               |\n",
            "|46751|0.0      |1911        |0.0               |\n",
            "|96128|1800000.0|1913        |0.0               |\n",
            "|28627|0.0      |1913        |1800000.0         |\n",
            "|56511|0.0      |1913        |1800000.0         |\n",
            "|56516|0.0      |1913        |1800000.0         |\n",
            "|98078|0.0      |1914        |0.0               |\n",
            "|28196|0.0      |1914        |0.0               |\n",
            "|5153 |0.0      |1914        |0.0               |\n",
            "|22943|0.0      |1914        |0.0               |\n",
            "+-----+---------+------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, year, max\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Convert release_date and extract year\n",
        "df_revenue = df.select(\n",
        "    'id',\n",
        "    'revenue',\n",
        "    to_date('release_date', 'yyyy-MM-dd').alias('release_date')\n",
        ").withColumn('release_year', year('release_date'))\n",
        "\n",
        "# Filter out rows with nulls in critical columns\n",
        "df_revenue = df_revenue.filter(col('id').isNotNull() & col('revenue').isNotNull() & col('release_year').isNotNull())\n",
        "\n",
        "# Define window\n",
        "windowRev = Window.partitionBy('release_year')\\\n",
        "                  .orderBy(col('revenue').desc())\\\n",
        "                  .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "# Add revenue difference\n",
        "df_revenue = df_revenue.withColumn(\n",
        "    'revenue_difference',\n",
        "    max(col('revenue')).over(windowRev) - col('revenue')\n",
        ")\n",
        "\n",
        "# Show final result\n",
        "df_revenue.select('id', 'revenue', 'release_year', 'revenue_difference').show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el46j8NYOVuH",
        "outputId": "cfb2a378-c80f-434d-e776-e240af34eabd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+------------+------------------+\n",
            "|id   |revenue  |release_year|revenue_difference|\n",
            "+-----+---------+------------+------------------+\n",
            "|70512|0.0      |1911        |0.0               |\n",
            "|46751|0.0      |1911        |0.0               |\n",
            "|96128|1800000.0|1913        |0.0               |\n",
            "|28627|0.0      |1913        |1800000.0         |\n",
            "|56511|0.0      |1913        |1800000.0         |\n",
            "|56516|0.0      |1913        |1800000.0         |\n",
            "|98078|0.0      |1914        |0.0               |\n",
            "|28196|0.0      |1914        |0.0               |\n",
            "|5153 |0.0      |1914        |0.0               |\n",
            "|22943|0.0      |1914        |0.0               |\n",
            "+-----+---------+------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Collect List\n",
        "\n",
        "# Step 1: Create the year column from release date\n",
        "df = df.withColumn('release_year',year('release_date'))\n",
        "\n",
        "# Step 2: Apply collect_list function to gather all occurrences\n",
        "df.filter(\"title=='The Lost World'\").groupby('title').agg(collect_list(\"release_year\")).show(1,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz11DVRMQfWj",
        "outputId": "939f2ce0-3b71-43eb-9cb6-6d42c0c44778"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------------------------+\n",
            "|title         |collect_list(release_year)          |\n",
            "+--------------+------------------------------------+\n",
            "|The Lost World|[1999, 2001, 1925, 1960, 1992, 1998]|\n",
            "+--------------+------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "(df\n",
        "    # 1️⃣  Filter first – touch as few rows as possible\n",
        "    .where(F.col(\"title\") == \"The Lost World\")\n",
        "\n",
        "    # 2️⃣  Project only the two columns you still need\n",
        "    .select(\n",
        "        \"title\",\n",
        "        F.year(\"release_date\").alias(\"release_year\")   # compute year on-the-fly\n",
        "    )\n",
        "\n",
        "    # 3️⃣  Single shuffle: group → collect_list\n",
        "    .groupBy(\"title\")\n",
        "    .agg(\n",
        "        F.sort_array(            # optional: deterministic order\n",
        "            F.collect_list(\"release_year\")\n",
        "        ).alias(\"release_years\")\n",
        "    )\n",
        "    .show(1, False)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXtTJ7E7Q2qT",
        "outputId": "4b1a2b4a-9764-4b80-e64d-5b8ad5f0e40d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+------------------------------------+\n",
            "|title         |release_years                       |\n",
            "+--------------+------------------------------------+\n",
            "|The Lost World|[1925, 1960, 1992, 1998, 1999, 2001]|\n",
            "+--------------+------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache if reused\n",
        "sub = df.where(F.col(\"title\") == \"The Lost World\").select(\"title\", F.year(\"release_date\").alias(\"release_year\"))\n",
        "sub.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaCuRdfvRDaB",
        "outputId": "6e0ba894-7378-40a8-96cf-9680a4975934"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[title: string, release_year: int]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition on the grouping key\n",
        "sub = sub.repartition(\"title\")"
      ],
      "metadata": {
        "id": "L5uFAwq0RIuI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sampling\n",
        "\n",
        "# Simple random sampling in PySpark without replacement\n",
        "df_sample = df.sample(False, 0.4, seed=11)\n",
        "print(f\"Sample size: {df_sample.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z79Wjz-gRP5L",
        "outputId": "ff9cd4b0-12c4-45e1-9c19-709952c20db4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size: 17627\n"
          ]
        }
      ]
    }
  ]
}